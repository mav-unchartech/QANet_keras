{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm #progress bar\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def word_tokenize(input_): #returns token version of input\n",
    "    input_nlp = nlp(input_)\n",
    "    return [token.text for token in input_nlp]\n",
    "\n",
    "def convert_idx(text, tokens): #returns spans for tokens within text\n",
    "    current = 0 #serves as a cursor so that the analysis doesn't go backward in text and saves times\n",
    "    #but what happens if you jump over a token? /!\\\n",
    "    spans = [] #list of spans (start, end)\n",
    "    for token in tokens:\n",
    "        current = text.find(token, current) #start index \n",
    "        if current < 0: #find returns - if not found\n",
    "            print(\"Token {} cannot be found\".format(token))\n",
    "            raise Exception()\n",
    "        spans.append((current, current + len(token)))\n",
    "        current += len(token) #the following search starts at the end of the current found token\n",
    "    return spans\n",
    "\n",
    "def process_file(filename, data_type, word_counter, char_counter):\n",
    "    print(\"Generating {} examples...\".format(data_type)) # data type is either train or dev\n",
    "    examples = [] #list with all the preprocessed SQUAD data about context and questions\n",
    "    eval_examples = {} #dict with answers\n",
    "    total = 0 #total number of questions over all articles\n",
    "    with open(filename, \"r\") as fh: #filname is SQUAD db\n",
    "        source = json.load(fh)\n",
    "        #explores Json structure to preprocess:\n",
    "        #(data(title, paragraph(context, qas(answers(answer_start, text), question, id))), version)\n",
    "        for article in tqdm(source[\"data\"]): #1st level\n",
    "            \n",
    "            for para in article[\"paragraphs\"]: #2nd level\n",
    "                #Each paragraph is a context and qas\n",
    "                #Here, preprocessing the context\n",
    "                context = para[\"context\"].replace(\"''\", '\" ').replace(\"``\", '\" ') #syntaxic preprocessing\n",
    "                context_tokens = word_tokenize(context) #list of all tokens from context\n",
    "                context_chars = [list(token) for token in context_tokens] #list of list of chars in context\n",
    "                spans = convert_idx(context, context_tokens) #list of spans for all tokens in context\n",
    "                for token in context_tokens:\n",
    "                    word_counter[token] += len(para[\"qas\"]) #collections.Counter() occurence\n",
    "                    #for each token in context, adds the total number of qas it is related to\n",
    "                    for char in token:\n",
    "                        char_counter[char] += len(para[\"qas\"]) #same over characters\n",
    "                        \n",
    "                #preprocessing qas, for each context several q-a pairs\n",
    "                for qa in para[\"qas\"]:\n",
    "                    if total >2000:\n",
    "                        break\n",
    "                    total += 1 #adding one to the total question count\n",
    "                    ques = qa[\"question\"].replace(\"''\", '\" ').replace(\"``\", '\" ') #syntaxic\n",
    "                    ques_tokens = word_tokenize(ques) #list, tokenized questions\n",
    "                    ques_chars = [list(token) for token in ques_tokens] #list of list of char in context\n",
    "                    for token in ques_tokens: #for each word in question\n",
    "                        word_counter[token] += 1 #the word in the question is related to the question, so add 1\n",
    "                        for char in token:\n",
    "                            char_counter[char] += 1 #same here\n",
    "                    y1s, y2s = [], [] #lisf of indices\n",
    "                    answer_texts = [] #list of all texts\n",
    "                    #for each answer now\n",
    "                    for answer in qa[\"answers\"]:\n",
    "                        answer_text = answer[\"text\"]\n",
    "                        answer_start = answer['answer_start']\n",
    "                        answer_end = answer_start + len(answer_text)\n",
    "                        answer_texts.append(answer_text)\n",
    "                        answer_span = [] #list of all spans' idx with start and end computed above\n",
    "                        #for each span now, which account for all tokens in context\n",
    "                        for idx, span in enumerate(spans):\n",
    "                            if not (answer_end <= span[0] or answer_start >= span[1]): #if the token is in the answer\n",
    "                                answer_span.append(idx)\n",
    "                        #for each answer, store first span idx and last span idx in y1 and y2\n",
    "                        y1, y2 = answer_span[0], answer_span[-1]\n",
    "                        y1s.append(y1) #list of indices of spans of words in context that is the first also in the answer\n",
    "                        y2s.append(y2)\n",
    "                    #end of context and question preprocessing, all is stored in example\n",
    "                    example = {\"context_tokens\": context_tokens, \"context_chars\": context_chars,\n",
    "                               \"ques_tokens\": ques_tokens,\n",
    "                               \"ques_chars\": ques_chars, \"y1s\": y1s, \"y2s\": y2s, \"id\": total}\n",
    "                    examples.append(example) #store all examples in a list\n",
    "                    #store each question info in a dict that identifies them by them number aka total\n",
    "                    eval_examples[str(total)] = {\n",
    "                        \"context\": context, \"spans\": spans, \"answers\": answer_texts, \"uuid\": qa[\"id\"]} #concatene les traitements\n",
    "        ########################\n",
    "        random.shuffle(examples)\n",
    "        ######################## Why? Does it improve perf? To test!\n",
    "        print(\"{} questions in total\".format(len(examples)))\n",
    "    return examples, eval_examples\n",
    "\n",
    "def get_embedding(counter, data_type, limit=-1, emb_file=None, size=None, vec_size=None):\n",
    "    print(\"Generating {} embedding...\".format(data_type))\n",
    "    embedding_dict = {} #dict of all embeddings to speed up process\n",
    "    filtered_elements = [k for k, v in counter.items() if v > limit]\n",
    "    #list of words for which the \"question related\" count is above limit\n",
    "    if emb_file is not None: #if glove has been provided: for words embedding\n",
    "        with open(emb_file, \"r\", encoding=\"utf-8\") as fh: #this is where they use glove\n",
    "            for line in tqdm(fh, total=size):#for each line in glove, which accounts for a word and its embedding\n",
    "                array = line.split() #line is a string, array is a list of all elements\n",
    "                word = \"\".join(array[0:-vec_size]) #word\n",
    "                vector = list(map(float, array[-vec_size:])) #embeddings vector\n",
    "                if word in counter and counter[word] > limit: #this is altready tested in filtered_elements\n",
    "                    embedding_dict[word] = vector #if word form glove is in the context, then store it in embeddings dict\n",
    "        print(\"{} / {} tokens have corresponding {} embedding vector\".format(\n",
    "            len(embedding_dict), len(filtered_elements), data_type))\n",
    "    else:\n",
    "        assert vec_size is not None\n",
    "        for token in filtered_elements: #all the other elements\n",
    "            embedding_dict[token] = [np.random.normal(scale=0.1) for _ in range(vec_size)]\n",
    "            #embedding vector is randomly generated\n",
    "        print(\"{} tokens have corresponding embedding vector\".format(\n",
    "            len(filtered_elements)))\n",
    "\n",
    "    NULL = \"--NULL--\"\n",
    "    OOV = \"--OOV--\"\n",
    "    #dict with token and its position in embedding dict\n",
    "    token2idx_dict = {token: idx for idx, token in enumerate(embedding_dict.keys(), 1)}\n",
    "    #initiate idx2token_dict\n",
    "    idx2token_dict={}\n",
    "    idx2token_dict[0]=NULL\n",
    "    idx2token_dict[len(embedding_dict)+1]=OOV\n",
    "    for k in token2idx_dict:\n",
    "        idx2token_dict[token2idx_dict[k]]=k #reverse token2idx_dict\n",
    "    #complete token2idx\n",
    "    token2idx_dict[NULL] = 0\n",
    "    token2idx_dict[OOV] = len(embedding_dict)+1\n",
    "    #initiate embedding_dict\n",
    "    embedding_dict[NULL] = [0. for _ in range(vec_size)] #for NULL word, the embedding is empty\n",
    "    embedding_dict[OOV] = np.random.random((1,vec_size))/2-0.25 #where do these figures come from?\n",
    "    #create idx2emb_dict with idx and embeddings vector\n",
    "    idx2emb_dict = {idx: embedding_dict[token] for token, idx in token2idx_dict.items()}\n",
    "    #emb_mat is a matrix of all embeddings for all indices\n",
    "    emb_mat = [idx2emb_dict[idx] for idx in range(len(idx2emb_dict))]\n",
    "    return emb_mat, token2idx_dict, idx2token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394/394 [01:16<00:00,  5.14it/s]\n",
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77811 questions in total\n",
      "Generating dev examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:09<00:00,  4.81it/s]\n",
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9788 questions in total\n",
      "Generating dev examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:10<00:00,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570 questions in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter #better than pure dict\n",
    "import numpy as np\n",
    "\n",
    "word_counter, char_counter = Counter(), Counter()\n",
    "\n",
    "#they all keep the same counters\n",
    "train_examples, train_eval = process_file('../../Database/train-v2.0.json', \"train\", word_counter, char_counter)\n",
    "dev_examples, dev_eval = process_file('../../Database/dev-v2.0.json', \"dev\", word_counter, char_counter)\n",
    "test_examples, test_eval = process_file('../../Database/dev-v2.0.json', \"dev\", word_counter, char_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save train_eval and dev_eval, might be used to save RAM!\n",
    "with open('dataset/train_eval.json', \"w\") as fh:\n",
    "    json.dump(train_eval, fh)\n",
    "with open('dataset/dev_eval.json','w') as fh:\n",
    "    json.dump(dev_eval,fh)\n",
    "with open('dataset/test_eval.json','w') as fh:\n",
    "    json.dump(test_eval,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 782/2200000 [00:00<04:41, 7815.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2196017/2200000 [03:17<00:00, 11146.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91587 / 111136 tokens have corresponding word embedding vector\n",
      "Generating char embedding...\n",
      "1425 tokens have corresponding embedding vector\n"
     ]
    }
   ],
   "source": [
    "glove_path = \"/home/unchartech001/Local_Resources/glove.6B/glove.6B.300d.txt\"\n",
    "word_emb_mat, word2idx_dict,id2word_dict = get_embedding(\n",
    "    word_counter, \"word\", emb_file=glove_path, size=int(2.2e6), vec_size=300)\n",
    "char_emb_mat, char2idx_dict, id2char_dict = get_embedding(\n",
    "    char_counter, \"char\", emb_file=None, size=None, vec_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_id2word = []\n",
    "for k in id2word_dict:\n",
    "    df_id2word.append([k, id2word_dict[k]]) #first save in a list all pairs of items and indices\n",
    "df_id2word = pd.DataFrame(df_id2word) #then into dataframe\n",
    "df_id2word.to_csv('id2word.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91589\n",
      "1426\n",
      "(91589, 300)\n"
     ]
    }
   ],
   "source": [
    "word_size = len(word_emb_mat) #length of embedding matrices\n",
    "char_input_size = len(char_emb_mat)-1 #idem characters\n",
    "print(word_size)\n",
    "print(char_input_size)\n",
    "word_mat = np.zeros((len(word_emb_mat),len(word_emb_mat[0])))\n",
    "for i, w in enumerate(word_emb_mat):\n",
    "    word_mat[i,:] = w\n",
    "print(word_mat.shape)\n",
    "np.save('word_emb_mat2.npy', word_mat) #saved as a numpy array and replicates word_emb_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(char2idx_dict)\n",
    "sorted(char_counter.items(), lambda x, y: cmp(x[1], y[1]), reverse=True)\n",
    "print(char_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77811/77811 [00:29<00:00, 2628.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss word: 88534\n",
      "miss char: 0\n",
      "over limit: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9788/9788 [00:03<00:00, 2669.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss word: 11241\n",
      "miss char: 0\n",
      "over limit: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [00:04<00:00, 2327.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss word: 13780\n",
      "miss char: 0\n",
      "over limit: 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def get_indexs(exa, word2idx_dict, char2idx_dict, cont_limit=400, ques_limit=50, ans_limit=30, char_limit=16):\n",
    "    n = len(exa) #total number of questions, >130k if all\n",
    "    miss_word = 0\n",
    "    miss_char = 0\n",
    "    overlimit = 0\n",
    "    #outputs are:\n",
    "    cont_index = np.zeros((n, cont_limit)) \n",
    "    ques_index = np.zeros((n, ques_limit))\n",
    "    cont_char_index = np.zeros((n, cont_limit, char_limit))\n",
    "    ques_char_index = np.zeros((n, ques_limit, char_limit))\n",
    "    cont_len = np.zeros((n, 1))\n",
    "    ques_len = np.zeros((n, 1))\n",
    "    y_start = np.zeros((n, cont_limit))\n",
    "    y_end = np.zeros((n, cont_limit))\n",
    "    qid = np.zeros((n))\n",
    "    \n",
    "    \n",
    "    #contexte\n",
    "    for i in tqdm(range(n-1)):\n",
    "        qid[i] = int(exa[i]['id'])\n",
    "        \n",
    "        contexts = exa[i]['context_tokens']\n",
    "        cont_len[i,0] = min(cont_limit, len(contexts))\n",
    "        for j,c in enumerate(contexts):\n",
    "            if j >= cont_limit:\n",
    "                break\n",
    "            if c in word2idx_dict:\n",
    "                cont_index[i,j] = word2idx_dict[c]\n",
    "            else:\n",
    "                miss_word += 1\n",
    "                cont_index[i,j] = word2idx_dict['--OOV--']\n",
    "        contexts_char = exa[i]['context_chars']\n",
    "        for j,c in enumerate(contexts_char):\n",
    "            if j >= cont_limit:\n",
    "                break\n",
    "            for j2,c2 in enumerate(c):\n",
    "                if j2 >= char_limit:\n",
    "                    break\n",
    "                if c2 in char2idx_dict:\n",
    "                    cont_char_index[i,j,j2] = char2idx_dict[c2]\n",
    "                else:\n",
    "                    miss_char += 1\n",
    "                    cont_char_index[i,j,j2] = char2idx_dict['--OOV--']\n",
    "        #answer\n",
    "        try:\n",
    "            st = exa[i]['y1s'][0]\n",
    "            ed = exa[i]['y2s'][0]\n",
    "            if st < cont_limit:\n",
    "                y_start[i, st] = 1\n",
    "            if ed < cont_limit:\n",
    "                if ed-st > ans_limit:\n",
    "                    y_end[i, st + ans_limit] = 1\n",
    "                    overlimit += 1\n",
    "                else:\n",
    "                    y_end[i, ed] = 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #question\n",
    "        contexts = exa[i]['ques_tokens']\n",
    "        ques_len[i, 0] = min(ques_limit, len(contexts))\n",
    "        for j,c in enumerate(contexts):\n",
    "            if j >= ques_limit:\n",
    "                break\n",
    "            if c in word2idx_dict:\n",
    "                ques_index[i, j] = word2idx_dict[c]\n",
    "            else:\n",
    "                miss_word += 1\n",
    "                ques_index[i, j] = word2idx_dict['--OOV--']\n",
    "        contexts_char = exa[i]['ques_chars']\n",
    "        for j,c in enumerate(contexts_char):\n",
    "            if j >= ques_limit:\n",
    "                break\n",
    "            for j2,c2 in enumerate(c):\n",
    "                if j2 >= char_limit:\n",
    "                    break\n",
    "                if c2 in char2idx_dict:\n",
    "                    ques_char_index[i, j, j2] = char2idx_dict[c2]\n",
    "                else:\n",
    "                    miss_char += 1\n",
    "                    ques_char_index[i,j,j2] = char2idx_dict['--OOV--']\n",
    "    print('miss word:', miss_word)\n",
    "    print('miss char:', miss_char)\n",
    "    print('over limit:', overlimit)\n",
    "        \n",
    "    return cont_index, ques_index, cont_char_index, ques_char_index, cont_len, ques_len, y_start, y_end, qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77811, 400)\n"
     ]
    }
   ],
   "source": [
    "#same reapeated 3 times for train dev and test\n",
    "#1st get indices\n",
    "#\n",
    "contw_input, quesw_input, contc_input,\\\n",
    "quesc_input, cont_len, ques_len, y_start,\\\n",
    "y_end, qid = get_indexs(train_examples, word2idx_dict, char2idx_dict)\n",
    "\n",
    "np.save('dataset/train_contw_input.npy',contw_input)\n",
    "np.save('dataset/train_quesw_input.npy',quesw_input)\n",
    "np.save('dataset/train_contc_input.npy',contc_input)\n",
    "np.save('dataset/train_quesc_input.npy',quesc_input)\n",
    "np.save('dataset/train_cont_len.npy',cont_len)\n",
    "np.save('dataset/train_ques_len.npy',ques_len)\n",
    "np.save('dataset/train_y_start.npy',y_start)\n",
    "np.save('dataset/train_y_end.npy',y_end)\n",
    "np.save('dataset/train_qid.npy',qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contw_input, quesw_input, contc_input, quesc_input, cont_len, ques_len, y_start, y_end, qid\\\n",
    "=get_indexs(dev_examples, word2idx_dict, char2idx_dict)\n",
    "\n",
    "np.save('dataset/dev_contw_input.npy',contw_input)\n",
    "np.save('dataset/dev_quesw_input.npy',quesw_input)\n",
    "np.save('dataset/dev_contc_input.npy',contc_input)\n",
    "np.save('dataset/dev_quesc_input.npy',quesc_input)\n",
    "np.save('dataset/dev_cont_len.npy',cont_len)\n",
    "np.save('dataset/dev_ques_len.npy',ques_len)\n",
    "np.save('dataset/dev_y_start.npy',y_start)\n",
    "np.save('dataset/dev_y_end.npy',y_end)\n",
    "np.save('dataset/dev_qid.npy',qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contw_input, quesw_input, contc_input, quesc_input, cont_len, ques_len, y_start, y_end, qid\\\n",
    "=get_indexs(test_examples, word2idx_dict, char2idx_dict)\n",
    "\n",
    "np.save('dataset/test_contw_input.npy',contw_input)\n",
    "np.save('dataset/test_quesw_input.npy',quesw_input)\n",
    "np.save('dataset/test_contc_input.npy',contc_input)\n",
    "np.save('dataset/test_quesc_input.npy',quesc_input)\n",
    "np.save('dataset/test_cont_len.npy',cont_len)\n",
    "np.save('dataset/test_ques_len.npy',ques_len)\n",
    "np.save('dataset/test_y_start.npy',y_start)\n",
    "np.save('dataset/test_y_end.npy',y_end)\n",
    "np.save('dataset/test_qid.npy',qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
