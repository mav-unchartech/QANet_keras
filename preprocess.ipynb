{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm #progress bar\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def word_tokenize(input_): #returns token version of input\n",
    "    input_nlp = nlp(input_)\n",
    "    return [token.text for token in input_nlp]\n",
    "\n",
    "def convert_idx(text, tokens): #returns spans for tokens within text\n",
    "    current = 0 #serves as a cursor so that the analysis doesn't go backward in text and saves times\n",
    "    #but what happens if you jump over a token? /!\\\n",
    "    spans = [] #list of spans (start, end)\n",
    "    for token in tokens:\n",
    "        current = text.find(token, current) #start index \n",
    "        if current < 0: #find returns - if not found\n",
    "            print(\"Token {} cannot be found\".format(token))\n",
    "            raise Exception()\n",
    "        spans.append((current, current + len(token)))\n",
    "        current += len(token) #the following search starts at the end of the current found token\n",
    "    return spans\n",
    "\n",
    "def process_file(filename, data_type, word_counter, char_counter):\n",
    "    print(\"Generating {} examples...\".format(data_type)) # data type is either train or dev\n",
    "    examples = [] #list with all the preprocessed SQUAD data about context and questions\n",
    "    eval_examples = {} #dict with answers\n",
    "    total = 0 #total number of questions over all articles\n",
    "    with open(filename, \"r\") as fh: #filname is SQUAD db\n",
    "        source = json.load(fh)\n",
    "        #explores Json structure to preprocess:\n",
    "        #(data(title, paragraph(context, qas(answers(answer_start, text), question, id))), version)\n",
    "        for article in tqdm(source[\"data\"]): #1st level\n",
    "            \n",
    "            for para in article[\"paragraphs\"]: #2nd level\n",
    "                #Each paragraph is a context and qas\n",
    "                #Here, preprocessing the context\n",
    "                context = para[\"context\"].replace(\"''\", '\" ').replace(\"``\", '\" ') #syntaxic preprocessing\n",
    "                context_tokens = word_tokenize(context) #list of all tokens from context\n",
    "                context_chars = [list(token) for token in context_tokens] #list of list of chars in context\n",
    "                spans = convert_idx(context, context_tokens) #list of spans for all tokens in context\n",
    "                for token in context_tokens:\n",
    "                    word_counter[token] += len(para[\"qas\"]) #collections.Counter() occurence\n",
    "                    #for each token in context, adds the total number of qas it is related to\n",
    "                    for char in token:\n",
    "                        char_counter[char] += len(para[\"qas\"]) #same over characters\n",
    "                        \n",
    "                #preprocessing qas, for each context several q-a pairs\n",
    "                for qa in para[\"qas\"]:\n",
    "                    if total >10: #added to limit total size\n",
    "                        break\n",
    "                    total += 1 #adding one to the total question count\n",
    "                    ques = qa[\"question\"].replace(\"''\", '\" ').replace(\"``\", '\" ') #syntaxic\n",
    "                    ques_tokens = word_tokenize(ques) #list, tokenized questions\n",
    "                    ques_chars = [list(token) for token in ques_tokens] #list of list of char in context\n",
    "                    for token in ques_tokens: #for each word in question\n",
    "                        word_counter[token] += 1 #the word in the question is related to the question, so add 1\n",
    "                        for char in token:\n",
    "                            char_counter[char] += 1 #same here\n",
    "                    y1s, y2s = [], [] #lisf of indices\n",
    "                    answer_texts = [] #list of all texts\n",
    "                    #for each answer now\n",
    "                    for answer in qa[\"answers\"]:\n",
    "                        answer_text = answer[\"text\"]\n",
    "                        answer_start = answer['answer_start']\n",
    "                        answer_end = answer_start + len(answer_text)\n",
    "                        answer_texts.append(answer_text)\n",
    "                        answer_span = [] #list of all spans' idx with start and end computed above\n",
    "                        #for each span now, which account for all tokens in context\n",
    "                        for idx, span in enumerate(spans):\n",
    "                            if not (answer_end <= span[0] or answer_start >= span[1]): #if the token is in the answer\n",
    "                                answer_span.append(idx)\n",
    "                        #for each answer, store first span idx and last span idx in y1 and y2\n",
    "                        y1, y2 = answer_span[0], answer_span[-1]\n",
    "                        y1s.append(y1) #list of indices of spans of words in context that is the first also in the answer\n",
    "                        y2s.append(y2)\n",
    "                    #end of context and question preprocessing, all is stored in example\n",
    "                    example = {\"context_tokens\": context_tokens, \"context_chars\": context_chars,\n",
    "                               \"ques_tokens\": ques_tokens,\n",
    "                               \"ques_chars\": ques_chars, \"y1s\": y1s, \"y2s\": y2s, \"id\": total}\n",
    "                    examples.append(example) #store all examples in a list\n",
    "                    #store each question info in a dict that identifies them by them number aka total\n",
    "                    eval_examples[str(total)] = {\n",
    "                        \"context\": context, \"spans\": spans, \"answers\": answer_texts, \"uuid\": qa[\"id\"]} #concatene les traitements\n",
    "        ########################\n",
    "        random.shuffle(examples)\n",
    "        ######################## Why? Does it improve perf? To test!\n",
    "        print(\"{} questions in total\".format(len(examples)))\n",
    "    return examples, eval_examples\n",
    "\n",
    "def get_embedding(counter, data_type, limit=-1, emb_file=None, size=None, vec_size=None):\n",
    "    print(\"Generating {} embedding...\".format(data_type))\n",
    "    embedding_dict = {} #dict of all embeddings to speed up process\n",
    "    filtered_elements = [k for k, v in counter.items() if v > limit]\n",
    "    #list of words for which the \"question related\" count is above limit\n",
    "    if emb_file is not None: #if glove has been provided: for words embedding\n",
    "        with open(emb_file, \"r\", encoding=\"utf-8\") as fh: #this is where they use glove\n",
    "            for line in tqdm(fh, total=size):#for each line in glove, which accounts for a word and its embedding\n",
    "                array = line.split() #line is a string, array is a list of all elements\n",
    "                word = \"\".join(array[0:-vec_size]) #word\n",
    "                vector = list(map(float, array[-vec_size:])) #embeddings vector\n",
    "                if word in counter and counter[word] > limit: #this is altready tested in filtered_elements\n",
    "                    embedding_dict[word] = vector #if word form glove is in the context, then store it in embeddings dict\n",
    "        print(\"{} / {} tokens have corresponding {} embedding vector\".format(\n",
    "            len(embedding_dict), len(filtered_elements), data_type))\n",
    "    else:\n",
    "        assert vec_size is not None\n",
    "        for token in filtered_elements: #all the other elements\n",
    "            embedding_dict[token] = [np.random.normal(scale=0.1) for _ in range(vec_size)]\n",
    "            #embedding vector is randomly generated\n",
    "        print(\"{} tokens have corresponding embedding vector\".format(\n",
    "            len(filtered_elements)))\n",
    "\n",
    "    NULL = \"--NULL--\"\n",
    "    OOV = \"--OOV--\"\n",
    "    #dict with token and its position in embedding dict\n",
    "    token2idx_dict = {token: idx for idx, token in enumerate(embedding_dict.keys(), 1)}\n",
    "    #initiate idx2token_dict\n",
    "    idx2token_dict={}\n",
    "    idx2token_dict[0]=NULL\n",
    "    idx2token_dict[len(embedding_dict)+1]=OOV\n",
    "    for k in token2idx_dict:\n",
    "        idx2token_dict[token2idx_dict[k]]=k #reverse token2idx_dict\n",
    "    #complete token2idx\n",
    "    token2idx_dict[NULL] = 0\n",
    "    token2idx_dict[OOV] = len(embedding_dict)+1\n",
    "    #initiate embedding_dict\n",
    "    embedding_dict[NULL] = [0. for _ in range(vec_size)] #for NULL word, the embedding is empty\n",
    "    embedding_dict[OOV] = np.random.random((1,vec_size))/2-0.25 #where do these figures come from?\n",
    "    #create idx2emb_dict with idx and embeddings vector\n",
    "    idx2emb_dict = {idx: embedding_dict[token] for token, idx in token2idx_dict.items()}\n",
    "    #emb_mat is a matrix of all embeddings for all indices\n",
    "    emb_mat = [idx2emb_dict[idx] for idx in range(len(idx2emb_dict))]\n",
    "    return emb_mat, token2idx_dict, idx2token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 442/442 [00:42<00:00, 10.42it/s]\n",
      "  3%|â–         | 1/35 [00:00<00:03,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 questions in total\n",
      "Generating dev examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:02<00:00, 12.55it/s]\n",
      "  0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 questions in total\n",
      "Generating dev examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:02<00:00, 12.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 questions in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter #better than pure dict\n",
    "import numpy as np\n",
    "\n",
    "word_counter, char_counter = Counter(), Counter()\n",
    "\n",
    "#they all keep the same counters\n",
    "train_examples, train_eval = process_file('../../Database/train-v2.0.json', \"train\", word_counter, char_counter)\n",
    "dev_examples, dev_eval = process_file('../../Database/dev-v2.0.json', \"dev\", word_counter, char_counter)\n",
    "test_examples, test_eval = process_file('../../Database/dev-v2.0.json', \"dev\", word_counter, char_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_eval and dev_eval, might be used to save RAM!\n",
    "with open('dataset/train_eval.json', \"w\") as fh:\n",
    "    json.dump(train_eval, fh)\n",
    "with open('dataset/dev_eval.json','w') as fh:\n",
    "    json.dump(dev_eval,fh)\n",
    "with open('dataset/test_eval.json','w') as fh:\n",
    "    json.dump(test_eval,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1203/2200000 [00:00<03:02, 12024.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 400001/2200000 [00:26<02:00, 14905.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43032 / 103636 tokens have corresponding word embedding vector\n",
      "Generating char embedding...\n",
      "1417 tokens have corresponding embedding vector\n"
     ]
    }
   ],
   "source": [
    "glove_path = \"/home/unchartech001/Local_Resources/glove.6B/glove.6B.300d.txt\"\n",
    "word_emb_mat, word2idx_dict,id2word_dict = get_embedding(\n",
    "    word_counter, \"word\", emb_file=glove_path, size=int(2.2e6), vec_size=300)\n",
    "char_emb_mat, char2idx_dict, id2char_dict = get_embedding(\n",
    "    char_counter, \"char\", emb_file=None, size=None, vec_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_id2word = []\n",
    "for k in id2word_dict:\n",
    "    df_id2word.append([k, id2word_dict[k]]) #first save in a list all pairs of items and indices\n",
    "df_id2word = pd.DataFrame(df_id2word) #then into dataframe\n",
    "df_id2word.to_csv('id2word.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43034\n",
      "1418\n",
      "(43034, 300)\n"
     ]
    }
   ],
   "source": [
    "word_size = len(word_emb_mat) #length of embedding matrices\n",
    "char_input_size = len(char_emb_mat)-1 #idem characters\n",
    "print(word_size)\n",
    "print(char_input_size)\n",
    "word_mat = np.zeros((len(word_emb_mat),len(word_emb_mat[0])))\n",
    "for i, w in enumerate(word_emb_mat):\n",
    "    word_mat[i,:] = w\n",
    "print(word_mat.shape)\n",
    "np.save('word_emb_mat2.npy', word_mat) #saved as a numpy array and replicates word_emb_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': 1, 'e': 2, 'y': 3, 'o': 4, 'n': 5, 'c': 6, 'Ã©': 7, 'G': 8, 'i': 9, 's': 10, 'l': 11, 'K': 12, 'w': 13, '-': 14, 'C': 15, 'a': 16, 'r': 17, 't': 18, '(': 19, '/': 20, 'b': 21, 'Ë': 22, 'Ëˆ': 23, 'j': 24, 'É’': 25, 'Éª': 26, 'Y': 27, 'O': 28, 'N': 29, ')': 30, 'S': 31, 'p': 32, 'm': 33, '4': 34, ',': 35, '1': 36, '9': 37, '8': 38, 'A': 39, 'g': 40, 'd': 41, 'u': 42, '.': 43, 'H': 44, 'T': 45, 'x': 46, 'h': 47, 'f': 48, 'v': 49, '0': 50, 'R': 51, '&': 52, 'D': 53, \"'\": 54, 'M': 55, 'L': 56, '2': 57, '3': 58, '\"': 59, 'z': 60, 'W': 61, '?': 62, 'I': 63, ' ': 64, 'k': 65, 'F': 66, 'J': 67, '5': 68, '6': 69, 'Ã ': 70, 'V': 71, 'P': 72, 'Z': 73, 'E': 74, ';': 75, 'q': 76, '7': 77, 'X': 78, 'U': 79, ':': 80, '$': 81, '[': 82, ']': 83, 'â€”': 84, 'Q': 85, '#': 86, 'â€“': 87, '%': 88, 'Ã¨': 89, 'Ã§': 90, 'Êƒ': 91, 'ÊŠ': 92, 'Ã¦': 93, '\\u200b': 94, 'Ê': 95, 'É‘': 96, 'Ìƒ': 97, 'É”': 98, 'É›': 99, 'Å„': 100, 'Å»': 101, 'Å¼': 102, 'Ã³': 103, 'Ã¼': 104, 'Å›': 105, 'Å‚': 106, 'Ã²': 107, 'Ã‰': 108, '!': 109, 'Ã´': 110, 'Â£': 111, 'Å': 112, 'Ã¤': 113, 'Ã­': 114, 'Å™': 115, 'Ã¡': 116, 'Å¯': 117, 'Ã¶': 118, 'Ã–': 119, 'Ã¬': 120, 'Ä«': 121, 'ä¿„': 122, 'åŠ›': 123, 'æ€': 124, 'è»': 125, 'æ°‘': 126, 'å…ƒ': 127, 'å¸¥': 128, 'åºœ': 129, 'Ãœ': 130, 'æ³•': 131, 'ç‹': 132, 'å¤§': 133, 'åœ‹': 134, 'å¸«': 135, 'å€™': 136, 'é¡¯': 137, 'Ä': 138, 'Å¡': 139, 'æ²': 140, 'è‹±': 141, 'â€™': 142, 'Ã£': 143, 'Â½': 144, '+': 145, 'Â¢': 146, 'ã‚¼': 147, 'ãƒ«': 148, 'ãƒ€': 149, 'ã®': 150, 'ä¼': 151, 'èª¬': 152, 'ãƒˆ': 153, 'ãƒ¯': 154, 'ã‚¤': 155, 'ãƒ©': 156, 'ãƒ—': 157, 'ãƒª': 158, 'ãƒ³': 159, 'ã‚»': 160, 'ã‚¹': 161, 'Å': 162, 'ÅŒ': 163, 'âˆ’': 164, 'â€œ': 165, 'â€': 166, 'æ±¶': 167, 'å·': 168, 'åœ°': 169, 'éœ‡': 170, 'Â°': 171, 'è€¿': 172, 'åº†': 173, 'å›½': 174, '>': 175, 'ç´«': 176, 'åª': 177, 'é“º': 178, 'æ°´': 179, 'åº“': 180, 'é‹ª': 181, 'åº«': 182, 'æœ±': 183, 'ç´¹': 184, 'ç¶­': 185, 'ç»': 186, 'ç»´': 187, 'Å«': 188, 'ä¹¦': 189, 'å‰‘': 190, 'å­': 191, 'â‚¬': 192, 'ã€Š': 193, 'ç¾': 194, 'å': 195, 'æ¢': 196, 'å¤': 197, 'é‡': 198, 'å»º': 199, 'å¯¹': 200, 'å£': 201, 'æ”¯': 202, 'æ´': 203, 'æ–¹': 204, 'æ¡ˆ': 205, 'ã€‹': 206, 'Â±': 207, 'è±†': 208, 'è…': 209, 'æ¸£': 210, 'æ ¡': 211, 'èˆ': 212, 'çˆ±': 213, 'çš„': 214, 'å¥‰': 215, 'çŒ®': 216, 'æ„›': 217, 'ç»': 218, '~': 219, 'Â¥': 220, 'è®©': 221, 'æµ': 222, 'ä¸': 223, 'æ¯': 224, 'åˆ˜': 225, 'å¤': 226, 'å·¥': 227, 'ç¨‹': 228, 'Ãª': 229, 'Â²': 230, 'ç´': 231, 'ç´„': 232, 'è¯': 233, 'åŸ ': 234, 'å¸ƒ': 235, 'é²': 236, 'å…‹': 237, 'æ—': 238, 'æ‹‰': 239, 'ç››': 240, 'æ‹¿': 241, 'é¨·': 242, 'ç¸£': 243, 'é•·': 244, 'å³¶': 245, 'æœ': 246, 'é²œ': 247, 'æ—': 248, 'ì¡°': 249, 'ì„ ': 250, 'ì¡±': 251, 'â¤': 252, 'É¹': 253, 'É™': 254, 'ÊŒ': 255, 'É¾': 256, 'Éœ': 257, 'Ã¯': 258, 'à¤§': 259, 'à¤°': 260, 'à¥': 261, 'à¤®': 262, 'á¹£': 263, 'Åš': 264, 'á¹‡': 265, 'á¹ƒ': 266, 'á¹…': 267, 'Ä€': 268, 'à¤¦': 269, 'à¥': 270, 'à¤•': 271, 'à¤–': 272, 'à¤ƒ': 273, 'á¸¥': 274, 'ç·£': 275, 'èµ·': 276, 'Ã±': 277, 'à¤¬': 278, 'à¥Š': 279, 'à¤¿': 280, '\\n': 281, 'å®‰': 282, 'æ¨‚': 283, 'æ·¨': 284, 'åœŸ': 285, 'á¹­': 286, 'à¤¯': 287, 'à¤¾': 288, 'à¤¨': 289, 'ç¦…': 290, 'è‡¨': 291, 'æ¸ˆ': 292, 'å®—': 293, 'æ›¹': 294, 'æ´': 295, 'å…¬': 296, 'á¹›': 297, 'á¸': 298, '\\u200d': 299, '\\u200c': 300, '*': 301, 'Åµ': 302, 'â€˜': 303, 'á»…': 304, 'áº¥': 305, 'Å©': 306, 'Ä': 307, 'áº¿': 308, 'Äƒ': 309, 'áº£': 310, 'Ã¸': 311, '\\u3000': 312, 'Î¦': 313, 'æ€»': 314, 'ç†': 315, 'Å': 316, 'Ä­': 317, 'Ï€': 318, 'Î¿': 319, 'Î»': 320, 'Ï': 321, 'Ãº': 322, 'Ã½': 323, 'Ï„': 324, 'Îµ': 325, 'Ï‡': 326, 'Î½': 327, 'Î¹': 328, 'Îº': 329, 'ÏŒ': 330, 'Ï‚': 331, 'Ä°': 332, 'â€¢': 333, '`': 334, 'Ã…': 335, 'Î‘': 336, 'Ï': 337, 'Î±': 338, 'Î¤': 339, 'Î³': 340, 'Î¬': 341, 'Î•': 342, 'Î´': 343, 'Ï…': 344, 'Î™': 345, 'Ï': 346, 'Î¼': 347, 'Î': 348, 'æ¥­': 349, 'å­¦': 350, 'Å ': 351, 'Ã«': 352, 'Ïƒ': 353, 'Î²': 354, 'Î¯': 355, 'Ï‰': 356, 'Ä‡': 357, 'Åº': 358, 'Ã¢': 359, 'Ø¯': 360, 'Ø±': 361, 'Ø¨': 362, 'Ø§': 363, '<': 364, 'Â§': 365, 'â„': 366, 'á¼€': 367, 'Î­': 368, 'Ã‡': 369, 'ËŒ': 370, 'É¡': 371, 'É': 372, 'Ì¯': 373, 'Ê': 374, 'Ì©': 375, 'â€¦': 376, 'Î¸': 377, 'á¼µ': 378, 'á½€': 379, 'Î¾': 380, 'Â·': 381, 'Î§': 382, '×': 383, 'Ö¸': 384, '×©': 385, 'Ö´': 386, '×': 387, '×™': 388, '×—': 389, 'Ö·': 390, '× ': 391, '×•': 392, 'Ö¼': 393, '×¦': 394, 'Ö°': 395, '×¨': 396, '×”': 397, '×“': 398, '×': 399, 'Ù†': 400, 'Øµ': 401, 'ÙŠ': 402, 'Ù‰': 403, 'Ù…': 404, 'Ø³': 405, 'Ø­': 406, 'á¹¢': 407, 'Ù„': 408, 'Ù': 409, 'Ø¬': 410, 'Ù‘': 411, 'Ø©': 412, 'ÛŒ': 413, 'Øª': 414, 'à¤ˆ': 415, 'à¤¸': 416, 'Ø¹': 417, 'Ø¦': 418, '\\u200e': 419, 'åŸº': 420, 'ç£': 421, 'å¾’': 422, 'Æ¡': 423, 'Ä‘': 424, 'á»‘': 425, 'á»“': 426, 'å‰': 427, 'åˆ©': 428, 'ä¸¹': 429, 'åˆ‡': 430, 'ã‚­': 431, 'ã‚·': 432, 'ã‚¿': 433, 'æ•™': 434, 'ã‚¯': 435, 'ãƒ': 436, 'ãƒ£': 437, 'ê¸°': 438, 'ë…': 439, 'êµ': 440, 'ë„': 441, 'ê·¸': 442, 'ë¦¬': 443, 'ìŠ¤': 444, 'Ğ’': 445, 'Ğµ': 446, 'Ğ»': 447, 'Ğ¸': 448, 'Ğº': 449, 'Ğ¾': 450, 'Ğ½': 451, 'Ñ': 452, 'Ğ¶': 453, 'Ñ': 454, 'Ñ‚': 455, 'Ğ²': 456, 'Ğ ': 457, 'Ñƒ': 458, 'Ñ…': 459, 'Ñ€': 460, 'Ğ°': 461, 'ÑŒ': 462, 'Ğ¿': 463, 'Ğ·': 464, 'Ğ¹': 465, 'Ğ¡': 466, 'Ğ¤': 467, 'Ğ´': 468, 'Ñ†': 469, 'Ñ‡': 470, 'Ğ±': 471, 'Å“': 472, 'Ã®': 473, 'Ğš': 474, 'Ğ¼': 475, 'Ê°': 476, 'âŸ¨': 477, 'â—Œ': 478, 'âŸ©': 479, 'Ê»': 480, 'Ë­': 481, 'É•': 482, 'Ê‚': 483, 'Ê±': 484, 'Í¡': 485, 'É¢': 486, 'á¶¢': 487, 'Ê˜': 488, 'Ç€': 489, 'Ç': 490, 'Çƒ': 491, 'Ç‚': 492, 'é™½': 493, 'Ïˆ': 494, 'Ê·': 495, 'Ã°': 496, 'Ê”': 497, 'É¦': 498, 'Ì¤': 499, 'â™ ': 500, 'á½‘': 501, 'Î®': 502, '=': 503, 'åŒ—': 504, 'æ–—': 505, 'å«': 506, 'æ˜Ÿ': 507, 'å¯¼': 508, 'èˆª': 509, 'ç³»': 510, 'ç»Ÿ': 511, 'è¡›': 512, 'å°': 513, 'çµ±': 514, 'Ä›': 515, 'Ç’': 516, 'Ç': 517, 'è¯•': 518, 'éªŒ': 519, 'è©¦': 520, 'é©—': 521, 'Ù‚': 522, 'Ùˆ': 523, '×§': 524, 'á½ ': 525, 'Ä“': 526, 'Î ': 527, 'Î·': 528, 'É«': 529, 'âˆ…': 530, 'Ë‘': 531, 'Ãµ': 532, 'Å¾': 533, 'Ã„': 534, 'É¤': 535, 'ÑŠ': 536, 'Ì': 537, 'Ñ‹': 538, 'Ğ¥': 539, 'Ã—': 540, '_': 541, 'â€²': 542, 'å—': 543, 'äº¬': 544, 'æ±Ÿ': 545, 'å¯§': 546, 'å®': 547, 'é‡‘': 548, 'é™µ': 549, 'å†¶': 550, 'åŸ': 551, 'è¶Š': 552, 'é‚‘': 553, 'ç§£': 554, 'åº·': 555, 'æ˜‡': 556, 'å·': 557, 'æ¸¤': 558, 'æ³¥': 559, 'å¤©': 560, 'ä¸‹': 561, 'æ·®': 562, 'æµ™': 563, 'æ±': 564, 'é•¿': 565, 'åŸŸ': 566, 'ä¸‰': 567, 'ç«': 568, 'ç‚‰': 569, 'é„´': 570, '×': 571, '×¤': 572, '×š': 573, '×–': 574, '×œ': 575, '×¢': 576, '×˜': 577, '×Ÿ': 578, '×²': 579, 'Ö¿': 580, 'Å£': 581, 'Ì¥': 582, 'â€³': 583, '\\ufeff': 584, 'Â´': 585, 'Ñ£': 586, 'Î£': 587, 'á¿–': 588, 'á¸±': 589, 'á¿†': 590, '^': 591, 'Ä': 592, 'Ä™': 593, 'Ğ‘': 594, 'Ğ': 595, 'Ğœ': 596, 'Ğˆ': 597, 'Ğ¢': 598, 'È™': 599, 'Å½': 600, 'Ì§': 601, 'Ä¼': 602, 'Ã': 603, 'Ì„': 604, 'Ä—': 605, 'Ê’': 606, 'áµ»': 607, 'â†’': 608, 'â€‘': 609, 'æŠ˜': 610, 'æ°µ': 611, 'äºº': 612, 'æ­Œ': 613, 'å¥³': 614, 'Çš': 615, 'é­': 616, 'èœ€': 617, 'æƒ°': 618, 'åŠ‡': 619, 'ä¸Š': 620, 'æœ‰': 621, 'å ‚': 622, 'ï¼Œ': 623, 'è‹': 624, 'æ­': 625, 'Ñ„': 626, 'Ã': 627, 'Ï†': 628, 'á¸—': 629, 'Ä±': 630, 'â‚¹': 631, '\\u202f': 632, 'Ğ“': 633, 'Î”': 634, 'Ğ„': 635, 'Ğ—': 636, 'Ğ˜': 637, 'Ø¥': 638, 'Ê¿': 639, 'Ã»': 640, 'â™¯': 641, '×ª': 642, '×¡': 643, 'Åˆ': 644, 'Ã¾': 645, 'â‚‚': 646, 'Ä…': 647, 'â‚¥': 648, 'á‰³': 649, 'áˆ‹': 650, 'áˆª': 651, 'ÃŸ': 652, 'Ã†': 653, 'Æ¿': 654, 'âŠ': 655, 'Ä‹': 656, 'Ä¡': 657, 'Âµ': 658, '\\u2009': 659, '{': 660, '}': 661, '|': 662, 'Ã¥': 663, 'Î¶': 664, 'ÅŸ': 665, 'Îœ': 666, 'Î˜': 667, 'á¿¦': 668, 'Î¡': 669, 'Î¥': 670, '@': 671, 'ĞŸ': 672, 'Ì': 673, 'Ñˆ': 674, 'Ìª': 675, 'à¤†': 676, 'à¤·': 677, 'å‰': 678, 'é‚£': 679, 'à¤£': 680, 'á‰¡': 681, 'áŠ•': 682, 'áˆ»': 683, 'áˆ‚': 684, 'á¼˜': 685, 'á½°': 686, 'á¼': 687, 'â€š': 688, 'Ã”': 689, 'â€º': 690, 'ãƒ•': 691, 'ã‚¡': 692, 'ãƒŸ': 693, 'ãƒ¼': 694, 'ã‚³': 695, 'ãƒ”': 696, 'ãƒ¥': 697, 'í˜„': 698, 'ëŒ€': 699, 'ì»´': 700, 'ë³´': 701, 'ì´': 702, 'ä»•': 703, 'æ§˜': 704, 'Ğ”': 705, 'å°': 706, 'æ‰': 707, 'á¼ˆ': 708, 'Î’': 709, 'á½¶': 710, 'á½¸': 711, 'Ã¿': 712, 'Å‘': 713, 'Ã“': 714, 'Ø®': 715, 'Ø£': 716, 'Ê¾': 717, 'á¼„': 718, '×‚': 719, 'Öµ': 720, 'Ù': 721, 'Ù’': 722, 'Ù': 723, 'Ê¼': 724, 'á¼¸': 725, 'á»‰': 726, 'êœ£': 727, 'á¼™': 728, 'ï¬': 729, 'ï¬‚': 730, 'æ‰‹': 731, 'ç¾½': 732, 'å…ˆ': 733, 'É£': 734, 'í•œ': 735, 'êµ­': 736, 'ì „': 737, 'ìŸ': 738, 'éŸ“': 739, 'æˆ°': 740, 'çˆ­': 741, 'í•´': 742, 'ë°©': 743, 'æŠ—': 744, 'ç¾': 745, 'æˆ˜': 746, 'äº‰': 747, 'é®®': 748, 'éŸ©': 749, 'Î“': 750, 'Î›': 751, 'Îš': 752, 'Ã¹': 753, 'Î—': 754, 'Î¨': 755, 'Ã': 756, 'ç¦': 757, 'è©±': 758, 'è¯': 759, 'Ì': 760, 'Ë¥': 761, 'Ë¨': 762, 'Ë©': 763, 'æ³‰': 764, 'æ¼³': 765, 'ç‰‡': 766, 'é–©': 767, 'èª': 768, 'é—½': 769, 'è¯­': 770, 'Ç': 771, 'Ç”': 772, 'ä½¬': 773, 'é™³': 774, 'æ”¿': 775, 'å…‰': 776, 'æ½®': 777, 'å¯©': 778, 'çŸ¥': 779, 'ç«¹': 780, 'æ–‡': 781, 'ç™½': 782, 'ç•°': 783, 'è®€': 784, 'éŸ³': 785, 'æ¼¢': 786, 'å­—': 787, 'æ›¿': 788, 'åª ': 789, 'å©': 790, 'é«˜': 791, 'æ‡¸': 792, 'æ¯‹': 793, 'å‘£': 794, 'å””': 795, 'ğªœ¶': 796, 'è‚‰': 797, 'å‘·': 798, 'é£Ÿ': 799, 'è¨€': 800, 'æ™®': 801, 'é€š': 802, 'è©': 803, 'å…¸': 804, 'Ãš': 805, 'ãƒ­': 806, 'ãƒ¬': 807, 'Â¡': 808, 'âˆš': 809, 'Ä†': 810, 'Ğ³': 811, 'ì˜ˆ': 812, 'ìˆ˜': 813, 'ì¥': 814, 'ë¡œ': 815, 'íšŒ': 816, 'ê³ ': 817, 'ë ¤': 818, 'íŒŒ': 819, 'ì‹ ': 820, 'í†µ': 821, 'í•©': 822, 'ë™': 823, 'á¸¤': 824, 'Â¿': 825, 'á€”': 826, 'á€¬': 827, 'á€¸': 828, 'á€': 829, 'Ù‡': 830, 'Ù': 831, 'Ğ•': 832, 'Ä': 833, 'É¥': 834, 'È›': 835, 'áº’': 836, 'Ø¡': 837, 'Ùƒ': 838, 'Ø´': 839, 'æ±‰': 840, 'á»¯': 841, 'æ²³': 842, 'æ¹–': 843, 'æ²–': 844, 'æ»‘': 845, 'Å‹': 846, 'ä¸­': 847, 'è´': 848, 'è¶': 849, 'è™«': 850, 'æ‰¹': 851, 'æŠŠ': 852, 'æ‡': 853, 'æ·': 854, 'çµ': 855, 'ç¶': 856, 'å…«': 857, 'åˆ†': 858, 'ç« ': 859, 'è‰': 860, 'éš¶': 861, 'éš¸': 862, 'ä»Š': 863, 'é “': 864, 'é¡¿': 865, 'æ¾': 866, 'æœ¨': 867, 'ç²': 868, 'ç±³': 869, 'åƒ': 870, 'ã“¾': 871, 'å¸¸': 872, 'ç”¨': 873, 'æ¨™': 874, 'æº–': 875, 'é«”': 876, 'è¡¨': 877, 'æ¬¡': 878, 'ç°': 879, 'ä»£': 880, 'å¹³': 881, 'è€ƒ': 882, 'ä»–': 883, 'å¥¹': 884, 'ç‰ ': 885, 'å®ƒ': 886, 'ç¥‚': 887, 'å’Œ': 888, 'å’Š': 889, 'é¾¢': 890, 'é½‰': 891, 'é¾˜': 892, 'ç±²': 893, 'é¬±': 894, 'æ†‚': 895, 'è±”': 896, 'é‡': 897, 'æŒ‘': 898, 'é±»': 899, 'ğªš¥': 900, 'é¾': 901, 'ğ ”»': 902, 'èˆˆ': 903, 'å—': 904, 'åˆ': 905, 'ç¥': 906, 'è©': 907, 'è–©': 908, 'è¨': 909, 'å': 910, 'å˜': 911, 'ç“¦': 912, 'ç“©': 913, 'ç³': 914, 'åœ•': 915, 'åœ–': 916, 'æ›¸': 917, 'é¤¨': 918, 'ç¤¾': 919, 'ä¼š': 920, 'ä¸»': 921, 'ä¹‰': 922, 'ç¤»': 923, 'å›': 924, 'å–œ': 925, 'åŒ': 926, 'é›™': 927, 'å»¿': 928, 'äºŒ': 929, 'å…': 930, 'åŒ': 931, 'å››': 932, 'ä¸ƒ': 933, 'é—¨': 934, 'å•': 935, 'é¡Œ': 936, 'é—®': 937, 'é¢˜': 938, 'åˆ': 939, 'ä½“': 940, 'èŠ‚': 941, 'ç¯€': 942, 'çŠ': 943, 'ç‘š': 944, 'èƒ¡': 945, 'å—': 946, 'å¡Š': 947, 'ç¯†': 948, 'æ–°': 949, 'æ—§': 950, 'å½“': 951, 'ãŸ': 952, 'ã¤': 953, 'ç«œ': 954, 'ä¾†': 955, 'æ¥': 956, 'é›²': 957, 'äº‘': 958, 'é›¨': 959, 'å ': 960, 'è¦†': 961, 'åƒ': 962, 'ë¬¼': 963, 'ì‚¬': 964, 'ëŒ': 965, 'ì¸': 966, 'í°': 967, 'ì‘': 968, 'ì„': 969, 'ì†Œ': 970, 'ì•„': 971, 'ë˜': 972, 'í•˜': 973, 'ë¹„': 974, 'ë¶€': 975, 'çˆ¶': 976, 'ë‚˜': 977, 'ë¼': 978, 'ë¦„': 979, 'é£': 980, 'åŠ¨': 981, 'ä»ª': 982, 'ãƒ‡': 983, 'ã‚¸': 984, 'ãƒ¢': 985, 'ãƒ': 986, 'ãƒ¡': 987, 'Ëš': 988, 'å¦¹': 989, 'ä»”': 990, 'æ™‚': 991, 'éƒ½': 992, 'é™¢': 993, 'åº': 994, 'ã„': 995, 'ã‚': 996, 'ã¯': 997, 'è‰¯': 998, 'Ğ': 999, 'Å†': 1000, 'Å': 1001, '×‘': 1002, '×’': 1003, '×¥': 1004, 'Ä§': 1005, 'å·´': 1006, 'æœ': 1007, 'å®‡': 1008, 'è¥¿': 1009, 'æˆ': 1010, 'ç»': 1011, 'æµ': 1012, 'æŠ€': 1013, 'æœ¯': 1014, 'å¼€': 1015, 'å‘': 1016, 'åŒº': 1017, 'äº§': 1018, 'ä¸š': 1019, 'ê€•': 1020, 'ï¸˜': 1021, 'ï¿½': 1022, 'ï½': 1023, 'ã€œ': 1024, 'à¹€': 1025, 'à¹': 1026, 'à¹‚': 1027, 'à¹ƒ': 1028, 'à¹„': 1029, 'à¸ª': 1030, 'à¸”': 1031, 'à¸‡': 1032, 'Ë¤': 1033, 'Ì€': 1034, 'è—': 1035, 'à½–': 1036, 'à½¼': 1037, 'à½‘': 1038, 'à¼‹': 1039, 'Ë§': 1040, 'å': 1041, 'è•ƒ': 1042, 'ç•ª': 1043, 'çƒ': 1044, 'æ–¯': 1045, 'ä¼¯': 1046, 'ç‰¹': 1047, 'å”': 1048, 'å¤': 1049, 'å¿’': 1050, 'å›¾': 1051, 'Å’': 1052, 'á ': 1053, 'á': 1054, 'á¦': 1055, 'á¯': 1056, 'á©': 1057, 'á¨': 1058, 'á±': 1059, 'á£': 1060, 'á³': 1061, 'á°': 1062, 'á¹': 1063, 'Ë€': 1064, 'à¤—': 1065, 'á¸·': 1066, 'à²š': 1067, 'à²¾': 1068, 'à²²': 1069, 'à³': 1070, 'à²•': 1071, 'à³': 1072, 'à²¯': 1073, 'à²°': 1074, 'É­': 1075, 'à¦ª': 1076, 'à¦¾': 1077, 'à¦²': 1078, 'à¦¸': 1079, 'à¦®': 1080, 'à§': 1081, 'à¦°': 1082, 'à¦œ': 1083, 'à¦¯': 1084, 'à²ª': 1085, 'à²¶': 1086, 'à²¿': 1087, 'à²®': 1088, 'à²¸': 1089, 'à²œ': 1090, 'Ê•': 1091, 'â‰¥': 1092, 'â„›': 1093, 'É¸': 1094, 'âˆ—': 1095, 'âˆˆ': 1096, 'â‰¡': 1097, 'âˆ–': 1098, 'Ê€': 1099, 'Ç¥': 1100, 'Çµ': 1101, 'Ã˜': 1102, 'Ã•': 1103, 'á¼”': 1104, 'Ã‚': 1105, 'á¼Œ': 1106, 'á¼': 1107, 'á¼°': 1108, 'á¼´': 1109, 'á¼­': 1110, 'á¼¡': 1111, 'ğ€': 1112, 'ğ€Š': 1113, 'ğ€': 1114, 'ğ€š': 1115, 'á½': 1116, 'á¼‘': 1117, 'á¼¶': 1118, 'á¿¶': 1119, 'á½²': 1120, 'æ­¦': 1121, 'å£«': 1122, 'å®¶': 1123, 'ä¾': 1124, 'å›£': 1125, 'åˆ¶': 1126, 'å¾': 1127, 'å¤·': 1128, 'å°†': 1129, 'å…¥': 1130, 'ã‚Š': 1131, 'ä¼': 1132, 'æ–¬': 1133, 'æ¨': 1134, 'ã¦': 1135, 'å¾¡': 1136, 'å…': 1137, 'ç¹”': 1138, 'ç”°': 1139, 'ç·': 1140, 'ä»‹': 1141, 'éƒ': 1142, 'ä¿¡': 1143, 'æµ¦': 1144, 'æŒ‰': 1145, 'é‡': 1146, 'é€¸': 1147, 'è¦‹': 1148, 'æ´²': 1149, 'è€¶': 1150, 'æ¥Š': 1151, 'å°': 1152, 'èˆ¹': 1153, 'å…µ': 1154, 'å®ˆ': 1155, 'ã‚‰': 1156, 'ãµ': 1157, 'ã‚‹': 1158, 'ã•': 1159, 'ã¶': 1160, 'ã²': 1161, 'ã‚€': 1162, 'é›†': 1163, 'è…¹': 1164, 'é“': 1165, 'è±Š': 1166, 'è‡£': 1167, 'ç§€': 1168, 'â‰ˆ': 1169, 'â‹…': 1170, 'É¨': 1171, 'é’': 1172, 'è—': 1173, 'ç¶ ': 1174, 'Çœ': 1175, 'ç·‘': 1176, 'ã‚°': 1177, 'á»': 1178, 'á»¥': 1179, 'à¸‚': 1180, 'à¸µ': 1181, 'à¸¢': 1182, 'à¸§': 1183, 'Ú¯': 1184, 'ÄŒ': 1185, 'â„–': 1186, 'Ã·': 1187, 'Ù”': 1188, 'Ø«': 1189, 'ÄŸ': 1190, 'Ø¸': 1191, 'Ø¶': 1192, 'Â¶': 1193, 'Ä ': 1194, 'Ä¦': 1195, 'áº“': 1196, 'á¸Œ': 1197, 'à¸£': 1198, 'à¸™': 1199, 'à¸¨': 1200, 'à¸²': 1201, 'à¸­': 1202, 'à¸´': 1203, 'à¸¥': 1204, 'à¸¡': 1205, 'á»': 1206, 'á»­': 1207, 'á»›': 1208, 'Æ°': 1209, 'Ê': 1210, 'Ë‡': 1211, 'Å¥': 1212, 'Ä': 1213, 'á”': 1214, 'á¾': 1215, 'á': 1216, 'â‚¤': 1217, 'Ø°': 1218, 'É¯': 1219, 'â™†': 1220, 'æµ·': 1221, 'â…“': 1222, 'å‰': 1223, 'ä¸œ': 1224, 'å¾Œ': 1225, 'æ¯”': 1226, 'è’²': 1227, 'å¥´': 1228, 'æª€': 1229, 'çŸ³': 1230, 'æ§': 1231, 'æ': 1232, 'é–': 1233, 'è†º': 1234, 'ç”«': 1235, 'å¼µ': 1236, 'å¥': 1237, 'ä½•': 1238, 'è‹—': 1239, 'åš': 1240, 'é­‚': 1241, 'é­„': 1242, 'ç¥': 1243, 'å»·': 1244, 'è­°': 1245, 'å°‰': 1246, 'éƒ¨': 1247, 'æ›²': 1248, 'äº”': 1249, 'éŠ–': 1250, 'è¶™': 1251, 'é': 1252, 'å‡¹': 1253, 'åŒ ': 1254, 'ä¸': 1255, 'ç·©': 1256, 'Ü©': 1257, 'Üª': 1258, 'Ü': 1259, 'Ü¢': 1260, 'Ü': 1261, 'âˆ': 1262, 'Â¼': 1263, 'äºœ': 1264, 'æˆ¦': 1265, 'æ—¥': 1266, 'äº‹': 1267, 'å¤‰': 1268, 'Ù': 1269, 'Ä’': 1270, 'á¼±': 1271, 'Ñ–': 1272, 'Ñ': 1273, 'Å­': 1274, 'Ñ—': 1275, 'ãƒ‘': 1276, 'ìŠˆ': 1277, 'í¼': 1278, 'ã‚«': 1279, 'ãƒƒ': 1280, 'Ä': 1281, 'ğ’Œ¦': 1282, 'ğ’Š•': 1283, 'ğ’ˆª': 1284, 'ğ’‚µ': 1285, 'Ê²': 1286, 'Ñµ': 1287, 'Ñ³': 1288, 'Ñ«': 1289, 'Ñ­': 1290, 'Ñ': 1291, 'Ñ§': 1292, 'Ñ©': 1293, 'Ñ‰': 1294, 'Ğ': 1295, 'Ñ': 1296, 'Ñ‘': 1297, 'Éµ': 1298, 'Ğ›': 1299, 'Ö¹': 1300, 'æ¸…': 1301, 'ÌŒ': 1302, 'æ˜': 1303, 'æœˆ': 1304, 'ä¹‹': 1305, 'åŠ‰': 1306, 'ä½': 1307, 'ç¾©': 1308, 'çš‡': 1309, 'å¸': 1310, 'è¦ª': 1311, 'è—©': 1312, 'æ¾„': 1313, 'å¼˜': 1314, 'æ¡“': 1315, 'é›': 1316, 'å·¡': 1317, 'æ’«': 1318, 'æ': 1319, 'ç¸½': 1320, 'å¾': 1321, 'é—œ': 1322, 'çš†': 1323, 'æƒ¡': 1324, 'å»š': 1325, 'é™‹': 1326, 'ç¿’': 1327, 'ã€‚': 1328, 'åª': 1329, 'å¯': 1330, 'æ–¼': 1331, 'é–€': 1332, 'å¸': 1333, 'å¢ƒ': 1334, 'çº¢': 1335, 'ç´…': 1336, 'åŒ…': 1337, 'â²¬': 1338, 'â²': 1339, 'â²™': 1340, 'â²“': 1341, 'Ì ': 1342, 'ğ’†³': 1343, 'ğ’„‘': 1344, 'ğ’Š’': 1345, 'â‚¯': 1346, 'Õ€': 1347, 'Õ¡': 1348, 'Õµ': 1349, 'Õ½': 1350, 'Õ¿': 1351, 'Õ¶': 1352, 'Ö': 1353, 'Õ«': 1354, 'ÕŠ': 1355, 'Ö€': 1356, 'Õ¯': 1357, 'Õ°': 1358, 'Ô¼': 1359, 'Õ¢': 1360, 'Ò·': 1361, 'Ò¶': 1362, 'Ò³': 1363, 'Ù€': 1364, 'Ó£': 1365, 'Ù¾': 1366, 'Êˆ': 1367, 'É³': 1368, 'É–': 1369, 'à¤ ': 1370, 'à¤¡': 1371, 'à¤ª': 1372, 'à¤¤': 1373, 'â‰ ': 1374, 'âŠ†': 1375, 'â„': 1376, 'è™Ÿ': 1377, 'è©”': 1378, 'å“‰': 1379, 'ä¹¾': 1380, 'é»‘': 1381, 'é¦¬': 1382, 'è•­': 1383, 'æœ­': 1384, 'å‰Œ': 1385, 'æŠ¹': 1386, 'å­›': 1387, 'è¿­': 1388, 'å…’': 1389, 'å¡”': 1390, 'å·²': 1391, 'åˆº': 1392, 'å²': 1393, 'ç§‰': 1394, 'ç›´': 1395, 'æŸ”': 1396, 'åš´': 1397, 'å¯¦': 1398, 'éˆ”': 1399, 'áº¡': 1400, 'áº±': 1401, 'áº§': 1402, 'á»‡': 1403, 'å°š': 1404, 'çœ': 1405, 'å¥': 1406, 'é–£': 1407, 'å­¸': 1408, 'ç¶“': 1409, 'ä¸–': 1410, 'å¤ª': 1411, 'ç¥–': 1412, 'æ¨': 1413, 'å¯†': 1414, 'æˆ': 1415, 'æš¦': 1416, 'á¿¬': 1417, '--NULL--': 0, '--OOV--': 1418}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must use keyword argument for key function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0ce17f82d77d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar2idx_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must use keyword argument for key function"
     ]
    }
   ],
   "source": [
    "print(char2idx_dict)\n",
    "sorted(char_counter.items(), lambda x, y: cmp(x[1], y[1]), reverse=True)\n",
    "print(char_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_indexs(exa, word2idx_dict, char2idx_dict, cont_limit=400, ques_limit=50, ans_limit=30, char_limit=16):\n",
    "    n = len(exa) #total number of questions, >130k if all\n",
    "    miss_word = 0\n",
    "    miss_char = 0\n",
    "    overlimit = 0\n",
    "    #outputs are:\n",
    "    cont_index = np.zeros((n, cont_limit)) \n",
    "    ques_index = np.zeros((n, ques_limit))\n",
    "    cont_char_index = np.zeros((n, cont_limit, char_limit))\n",
    "    ques_char_index = np.zeros((n, ques_limit, char_limit))\n",
    "    cont_len = np.zeros((n, 1))\n",
    "    ques_len = np.zeros((n, 1))\n",
    "    y_start = np.zeros((n, cont_limit))\n",
    "    y_end = np.zeros((n, cont_limit))\n",
    "    qid = np.zeros((n))\n",
    "    \n",
    "    \n",
    "    #contexte\n",
    "    for i in tqdm(range(n-1)):\n",
    "        qid[i] = int(exa[i]['id'])\n",
    "        \n",
    "        contexts = exa[i]['context_tokens']\n",
    "        cont_len[i,0] = min(cont_limit, len(contexts))\n",
    "        for j,c in enumerate(contexts):\n",
    "            if j >= cont_limit:\n",
    "                break\n",
    "            if c in word2idx_dict:\n",
    "                cont_index[i,j] = word2idx_dict[c]\n",
    "            else:\n",
    "                miss_word += 1\n",
    "                cont_index[i,j] = word2idx_dict['--OOV--']\n",
    "        contexts_char = exa[i]['context_chars']\n",
    "        for j,c in enumerate(contexts_char):\n",
    "            if j >= cont_limit:\n",
    "                break\n",
    "            for j2,c2 in enumerate(c):\n",
    "                if j2 >= char_limit:\n",
    "                    break\n",
    "                if c2 in char2idx_dict:\n",
    "                    cont_char_index[i,j,j2] = char2idx_dict[c2]\n",
    "                else:\n",
    "                    miss_char += 1\n",
    "                    cont_char_index[i,j,j2] = char2idx_dict['--OOV--']\n",
    "        #answer\n",
    "        try:\n",
    "            st = exa[i]['y1s'][0]\n",
    "            ed = exa[i]['y2s'][0]\n",
    "            if st < cont_limit:\n",
    "                y_start[i, st] = 1\n",
    "            if ed < cont_limit:\n",
    "                if ed-st > ans_limit:\n",
    "                    y_end[i, st + ans_limit] = 1\n",
    "                    overlimit += 1\n",
    "                else:\n",
    "                    y_end[i, ed] = 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #question\n",
    "        contexts = exa[i]['ques_tokens']\n",
    "        ques_len[i, 0] = min(ques_limit, len(contexts))\n",
    "        for j,c in enumerate(contexts):\n",
    "            if j >= ques_limit:\n",
    "                break\n",
    "            if c in word2idx_dict:\n",
    "                ques_index[i, j] = word2idx_dict[c]\n",
    "            else:\n",
    "                miss_word += 1\n",
    "                ques_index[i, j] = word2idx_dict['--OOV--']\n",
    "        contexts_char = exa[i]['ques_chars']\n",
    "        for j,c in enumerate(contexts_char):\n",
    "            if j >= ques_limit:\n",
    "                break\n",
    "            for j2,c2 in enumerate(c):\n",
    "                if j2 >= char_limit:\n",
    "                    break\n",
    "                if c2 in char2idx_dict:\n",
    "                    ques_char_index[i, j, j2] = char2idx_dict[c2]\n",
    "                else:\n",
    "                    miss_char += 1\n",
    "                    ques_char_index[i,j,j2] = char2idx_dict['--OOV--']\n",
    "    print('miss word:', miss_word)\n",
    "    print('miss char:', miss_char)\n",
    "    print('over limit:', overlimit)\n",
    "        \n",
    "    return cont_index, ques_index, cont_char_index, ques_char_index, cont_len, ques_len, y_start, y_end, qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1664.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss word: 313\n",
      "miss char: 0\n",
      "over limit: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#same reapeated 3 times for train dev and test\n",
    "#1st get indices\n",
    "#\n",
    "contw_input, quesw_input, contc_input,\\\n",
    "quesc_input, cont_len, ques_len, y_start,\\\n",
    "y_end, qid = get_indexs(train_examples, word2idx_dict, char2idx_dict)\n",
    "\n",
    "np.save('dataset/train_contw_input.npy',contw_input)\n",
    "np.save('dataset/train_quesw_input.npy',quesw_input)\n",
    "np.save('dataset/train_contc_input.npy',contc_input)\n",
    "np.save('dataset/train_quesc_input.npy',quesc_input)\n",
    "np.save('dataset/train_cont_len.npy',cont_len)\n",
    "np.save('dataset/train_ques_len.npy',ques_len)\n",
    "np.save('dataset/train_y_start.npy',y_start)\n",
    "np.save('dataset/train_y_end.npy',y_end)\n",
    "np.save('dataset/train_qid.npy',qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1390.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss word: 399\n",
      "miss char: 0\n",
      "over limit: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "contw_input, quesw_input, contc_input, quesc_input, cont_len, ques_len, y_start, y_end, qid\\\n",
    "=get_indexs(dev_examples, word2idx_dict, char2idx_dict)\n",
    "\n",
    "np.save('dataset/dev_contw_input.npy',contw_input)\n",
    "np.save('dataset/dev_quesw_input.npy',quesw_input)\n",
    "np.save('dataset/dev_contc_input.npy',contc_input)\n",
    "np.save('dataset/dev_quesc_input.npy',quesc_input)\n",
    "np.save('dataset/dev_cont_len.npy',cont_len)\n",
    "np.save('dataset/dev_ques_len.npy',ques_len)\n",
    "np.save('dataset/dev_y_start.npy',y_start)\n",
    "np.save('dataset/dev_y_end.npy',y_end)\n",
    "np.save('dataset/dev_qid.npy',qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1414.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss word: 398\n",
      "miss char: 0\n",
      "over limit: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "contw_input, quesw_input, contc_input, quesc_input, cont_len, ques_len, y_start, y_end, qid\\\n",
    "=get_indexs(test_examples, word2idx_dict, char2idx_dict)\n",
    "\n",
    "np.save('dataset/test_contw_input.npy',contw_input)\n",
    "np.save('dataset/test_quesw_input.npy',quesw_input)\n",
    "np.save('dataset/test_contc_input.npy',contc_input)\n",
    "np.save('dataset/test_quesc_input.npy',quesc_input)\n",
    "np.save('dataset/test_cont_len.npy',cont_len)\n",
    "np.save('dataset/test_ques_len.npy',ques_len)\n",
    "np.save('dataset/test_y_start.npy',y_start)\n",
    "np.save('dataset/test_y_end.npy',y_end)\n",
    "np.save('dataset/test_qid.npy',qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
