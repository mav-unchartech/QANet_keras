{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm #progress bar\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def word_tokenize(input_): #returns token version of input\n",
    "    input_nlp = nlp(input_)\n",
    "    return [token.text for token in input_nlp]\n",
    "\n",
    "def convert_idx(text, tokens): #returns spans for tokens within text\n",
    "    current = 0 #serves as a cursor so that the analysis doesn't go backward in text and saves times\n",
    "    #but what happens if you jump over a token? /!\\\n",
    "    spans = [] #list of spans (start, end)\n",
    "    for token in tokens:\n",
    "        current = text.find(token, current) #start index \n",
    "        if current < 0: #find returns - if not found\n",
    "            print(\"Token {} cannot be found\".format(token))\n",
    "            raise Exception()\n",
    "        spans.append((current, current + len(token)))\n",
    "        current += len(token) #the following search starts at the end of the current found token\n",
    "    return spans\n",
    "\n",
    "def process_file(filename, data_type, word_counter, char_counter):\n",
    "    print(\"Generating {} examples...\".format(data_type)) # data type is either train or dev\n",
    "    examples = [] #list with all the preprocessed SQUAD data about context and questions\n",
    "    eval_examples = {} #dict with answers\n",
    "    total = 0 #total number of questions over all articles\n",
    "    with open(filename, \"r\") as fh: #filname is SQUAD db\n",
    "        source = json.load(fh)\n",
    "        #explores Json structure to preprocess:\n",
    "        #(data(title, paragraph(context, qas(answers(answer_start, text), question, id))), version)\n",
    "        for article in tqdm(source[\"data\"]): #1st level\n",
    "            \n",
    "            for para in article[\"paragraphs\"]: #2nd level\n",
    "                #Each paragraph is a context and qas\n",
    "                #Here, preprocessing the context\n",
    "                context = para[\"context\"].replace(\"''\", '\" ').replace(\"``\", '\" ') #syntaxic preprocessing\n",
    "                context_tokens = word_tokenize(context) #list of all tokens from context\n",
    "                context_chars = [list(token) for token in context_tokens] #list of list of chars in context\n",
    "                spans = convert_idx(context, context_tokens) #list of spans for all tokens in context\n",
    "                for token in context_tokens:\n",
    "                    word_counter[token] += len(para[\"qas\"]) #collections.Counter() occurence\n",
    "                    #for each token in context, adds the total number of qas it is related to\n",
    "                    for char in token:\n",
    "                        char_counter[char] += len(para[\"qas\"]) #same over characters\n",
    "                        \n",
    "                #preprocessing qas, for each context several q-a pairs\n",
    "                for qa in para[\"qas\"]:\n",
    "                    if total >10: #added to limit total size\n",
    "                        break\n",
    "                    total += 1 #adding one to the total question count\n",
    "                    ques = qa[\"question\"].replace(\"''\", '\" ').replace(\"``\", '\" ') #syntaxic\n",
    "                    ques_tokens = word_tokenize(ques) #list, tokenized questions\n",
    "                    ques_chars = [list(token) for token in ques_tokens] #list of list of char in context\n",
    "                    for token in ques_tokens: #for each word in question\n",
    "                        word_counter[token] += 1 #the word in the question is related to the question, so add 1\n",
    "                        for char in token:\n",
    "                            char_counter[char] += 1 #same here\n",
    "                    y1s, y2s = [], [] #lisf of indices\n",
    "                    answer_texts = [] #list of all texts\n",
    "                    #for each answer now\n",
    "                    for answer in qa[\"answers\"]:\n",
    "                        answer_text = answer[\"text\"]\n",
    "                        answer_start = answer['answer_start']\n",
    "                        answer_end = answer_start + len(answer_text)\n",
    "                        answer_texts.append(answer_text)\n",
    "                        answer_span = [] #list of all spans' idx with start and end computed above\n",
    "                        #for each span now, which account for all tokens in context\n",
    "                        for idx, span in enumerate(spans):\n",
    "                            if not (answer_end <= span[0] or answer_start >= span[1]): #if the token is in the answer\n",
    "                                answer_span.append(idx)\n",
    "                        #for each answer, store first span idx and last span idx in y1 and y2\n",
    "                        y1, y2 = answer_span[0], answer_span[-1]\n",
    "                        y1s.append(y1) #list of indices of spans of words in context that is the first also in the answer\n",
    "                        y2s.append(y2)\n",
    "                    #end of context and question preprocessing, all is stored in example\n",
    "                    example = {\"context_tokens\": context_tokens, \"context_chars\": context_chars,\n",
    "                               \"ques_tokens\": ques_tokens,\n",
    "                               \"ques_chars\": ques_chars, \"y1s\": y1s, \"y2s\": y2s, \"id\": total}\n",
    "                    examples.append(example) #store all examples in a list\n",
    "                    #store each question info in a dict that identifies them by them number aka total\n",
    "                    eval_examples[str(total)] = {\n",
    "                        \"context\": context, \"spans\": spans, \"answers\": answer_texts, \"uuid\": qa[\"id\"]} #concatene les traitements\n",
    "        ########################\n",
    "        random.shuffle(examples)\n",
    "        ######################## Why? Does it improve perf? To test!\n",
    "        print(\"{} questions in total\".format(len(examples)))\n",
    "    return examples, eval_examples\n",
    "\n",
    "def get_embedding(counter, data_type, limit=-1, emb_file=None, size=None, vec_size=None):\n",
    "    print(\"Generating {} embedding...\".format(data_type))\n",
    "    embedding_dict = {} #dict of all embeddings to speed up process\n",
    "    filtered_elements = [k for k, v in counter.items() if v > limit]\n",
    "    #list of words for which the \"question related\" count is above limit\n",
    "    if emb_file is not None: #if glove has been provided: for words embedding\n",
    "        with open(emb_file, \"r\", encoding=\"utf-8\") as fh: #this is where they use glove\n",
    "            for line in tqdm(fh, total=size):#for each line in glove, which accounts for a word and its embedding\n",
    "                array = line.split() #line is a string, array is a list of all elements\n",
    "                word = \"\".join(array[0:-vec_size]) #word\n",
    "                vector = list(map(float, array[-vec_size:])) #embeddings vector\n",
    "                if word in counter and counter[word] > limit: #this is altready tested in filtered_elements\n",
    "                    embedding_dict[word] = vector #if word form glove is in the context, then store it in embeddings dict\n",
    "        print(\"{} / {} tokens have corresponding {} embedding vector\".format(\n",
    "            len(embedding_dict), len(filtered_elements), data_type))\n",
    "    else:\n",
    "        assert vec_size is not None\n",
    "        for token in filtered_elements: #all the other elements\n",
    "            embedding_dict[token] = [np.random.normal(scale=0.1) for _ in range(vec_size)]\n",
    "            #embedding vector is randomly generated\n",
    "        print(\"{} tokens have corresponding embedding vector\".format(\n",
    "            len(filtered_elements)))\n",
    "\n",
    "    NULL = \"--NULL--\"\n",
    "    OOV = \"--OOV--\"\n",
    "    #dict with token and its position in embedding dict\n",
    "    token2idx_dict = {token: idx for idx, token in enumerate(embedding_dict.keys(), 1)}\n",
    "    #initiate idx2token_dict\n",
    "    idx2token_dict={}\n",
    "    idx2token_dict[0]=NULL\n",
    "    idx2token_dict[len(embedding_dict)+1]=OOV\n",
    "    for k in token2idx_dict:\n",
    "        idx2token_dict[token2idx_dict[k]]=k #reverse token2idx_dict\n",
    "    #complete token2idx\n",
    "    token2idx_dict[NULL] = 0\n",
    "    token2idx_dict[OOV] = len(embedding_dict)+1\n",
    "    #initiate embedding_dict\n",
    "    embedding_dict[NULL] = [0. for _ in range(vec_size)] #for NULL word, the embedding is empty\n",
    "    embedding_dict[OOV] = np.random.random((1,vec_size))/2-0.25 #where do these figures come from?\n",
    "    #create idx2emb_dict with idx and embeddings vector\n",
    "    idx2emb_dict = {idx: embedding_dict[token] for token, idx in token2idx_dict.items()}\n",
    "    #emb_mat is a matrix of all embeddings for all indices\n",
    "    emb_mat = [idx2emb_dict[idx] for idx in range(len(idx2emb_dict))]\n",
    "    return emb_mat, token2idx_dict, idx2token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:42<00:00, 10.42it/s]\n",
      "  3%|▎         | 1/35 [00:00<00:03,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 questions in total\n",
      "Generating dev examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:02<00:00, 12.55it/s]\n",
      "  0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 questions in total\n",
      "Generating dev examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:02<00:00, 12.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 questions in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter #better than pure dict\n",
    "import numpy as np\n",
    "\n",
    "word_counter, char_counter = Counter(), Counter()\n",
    "\n",
    "#they all keep the same counters\n",
    "train_examples, train_eval = process_file('../../Database/train-v2.0.json', \"train\", word_counter, char_counter)\n",
    "dev_examples, dev_eval = process_file('../../Database/dev-v2.0.json', \"dev\", word_counter, char_counter)\n",
    "test_examples, test_eval = process_file('../../Database/dev-v2.0.json', \"dev\", word_counter, char_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_eval and dev_eval, might be used to save RAM!\n",
    "with open('dataset/train_eval.json', \"w\") as fh:\n",
    "    json.dump(train_eval, fh)\n",
    "with open('dataset/dev_eval.json','w') as fh:\n",
    "    json.dump(dev_eval,fh)\n",
    "with open('dataset/test_eval.json','w') as fh:\n",
    "    json.dump(test_eval,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1203/2200000 [00:00<03:02, 12024.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 400001/2200000 [00:26<02:00, 14905.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43032 / 103636 tokens have corresponding word embedding vector\n",
      "Generating char embedding...\n",
      "1417 tokens have corresponding embedding vector\n"
     ]
    }
   ],
   "source": [
    "glove_path = \"/home/unchartech001/Local_Resources/glove.6B/glove.6B.300d.txt\"\n",
    "word_emb_mat, word2idx_dict,id2word_dict = get_embedding(\n",
    "    word_counter, \"word\", emb_file=glove_path, size=int(2.2e6), vec_size=300)\n",
    "char_emb_mat, char2idx_dict, id2char_dict = get_embedding(\n",
    "    char_counter, \"char\", emb_file=None, size=None, vec_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_id2word = []\n",
    "for k in id2word_dict:\n",
    "    df_id2word.append([k, id2word_dict[k]]) #first save in a list all pairs of items and indices\n",
    "df_id2word = pd.DataFrame(df_id2word) #then into dataframe\n",
    "df_id2word.to_csv('id2word.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43034\n",
      "1418\n",
      "(43034, 300)\n"
     ]
    }
   ],
   "source": [
    "word_size = len(word_emb_mat) #length of embedding matrices\n",
    "char_input_size = len(char_emb_mat)-1 #idem characters\n",
    "print(word_size)\n",
    "print(char_input_size)\n",
    "word_mat = np.zeros((len(word_emb_mat),len(word_emb_mat[0])))\n",
    "for i, w in enumerate(word_emb_mat):\n",
    "    word_mat[i,:] = w\n",
    "print(word_mat.shape)\n",
    "np.save('word_emb_mat2.npy', word_mat) #saved as a numpy array and replicates word_emb_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': 1, 'e': 2, 'y': 3, 'o': 4, 'n': 5, 'c': 6, 'é': 7, 'G': 8, 'i': 9, 's': 10, 'l': 11, 'K': 12, 'w': 13, '-': 14, 'C': 15, 'a': 16, 'r': 17, 't': 18, '(': 19, '/': 20, 'b': 21, 'ː': 22, 'ˈ': 23, 'j': 24, 'ɒ': 25, 'ɪ': 26, 'Y': 27, 'O': 28, 'N': 29, ')': 30, 'S': 31, 'p': 32, 'm': 33, '4': 34, ',': 35, '1': 36, '9': 37, '8': 38, 'A': 39, 'g': 40, 'd': 41, 'u': 42, '.': 43, 'H': 44, 'T': 45, 'x': 46, 'h': 47, 'f': 48, 'v': 49, '0': 50, 'R': 51, '&': 52, 'D': 53, \"'\": 54, 'M': 55, 'L': 56, '2': 57, '3': 58, '\"': 59, 'z': 60, 'W': 61, '?': 62, 'I': 63, ' ': 64, 'k': 65, 'F': 66, 'J': 67, '5': 68, '6': 69, 'à': 70, 'V': 71, 'P': 72, 'Z': 73, 'E': 74, ';': 75, 'q': 76, '7': 77, 'X': 78, 'U': 79, ':': 80, '$': 81, '[': 82, ']': 83, '—': 84, 'Q': 85, '#': 86, '–': 87, '%': 88, 'è': 89, 'ç': 90, 'ʃ': 91, 'ʊ': 92, 'æ': 93, '\\u200b': 94, 'ʁ': 95, 'ɑ': 96, '̃': 97, 'ɔ': 98, 'ɛ': 99, 'ń': 100, 'Ż': 101, 'ż': 102, 'ó': 103, 'ü': 104, 'ś': 105, 'ł': 106, 'ò': 107, 'É': 108, '!': 109, 'ô': 110, '£': 111, 'Ł': 112, 'ä': 113, 'í': 114, 'ř': 115, 'á': 116, 'ů': 117, 'ö': 118, 'Ö': 119, 'ì': 120, 'ī': 121, '俄': 122, '力': 123, '思': 124, '軍': 125, '民': 126, '元': 127, '帥': 128, '府': 129, 'Ü': 130, '法': 131, '王': 132, '大': 133, '國': 134, '師': 135, '候': 136, '顯': 137, 'ā': 138, 'š': 139, '沐': 140, '英': 141, '’': 142, 'ã': 143, '½': 144, '+': 145, '¢': 146, 'ゼ': 147, 'ル': 148, 'ダ': 149, 'の': 150, '伝': 151, '説': 152, 'ト': 153, 'ワ': 154, 'イ': 155, 'ラ': 156, 'プ': 157, 'リ': 158, 'ン': 159, 'セ': 160, 'ス': 161, 'ō': 162, 'Ō': 163, '−': 164, '“': 165, '”': 166, '汶': 167, '川': 168, '地': 169, '震': 170, '°': 171, '耿': 172, '庆': 173, '国': 174, '>': 175, '紫': 176, '坪': 177, '铺': 178, '水': 179, '库': 180, '鋪': 181, '庫': 182, '朱': 183, '紹': 184, '維': 185, '绍': 186, '维': 187, 'ū': 188, '书': 189, '剑': 190, '子': 191, '€': 192, '《': 193, '灾': 194, '后': 195, '恢': 196, '复': 197, '重': 198, '建': 199, '对': 200, '口': 201, '支': 202, '援': 203, '方': 204, '案': 205, '》': 206, '±': 207, '豆': 208, '腐': 209, '渣': 210, '校': 211, '舍': 212, '爱': 213, '的': 214, '奉': 215, '献': 216, '愛': 217, '獻': 218, '~': 219, '¥': 220, '让': 221, '流': 222, '不': 223, '息': 224, '刘': 225, '坤': 226, '工': 227, '程': 228, 'ê': 229, '²': 230, '紐': 231, '約': 232, '華': 233, '埠': 234, '布': 235, '鲁': 236, '克': 237, '林': 238, '拉': 239, '盛': 240, '拿': 241, '騷': 242, '縣': 243, '長': 244, '島': 245, '朝': 246, '鲜': 247, '族': 248, '조': 249, '선': 250, '족': 251, '❤': 252, 'ɹ': 253, 'ə': 254, 'ʌ': 255, 'ɾ': 256, 'ɜ': 257, 'ï': 258, 'ध': 259, 'र': 260, '्': 261, 'म': 262, 'ṣ': 263, 'Ś': 264, 'ṇ': 265, 'ṃ': 266, 'ṅ': 267, 'Ā': 268, 'द': 269, 'ु': 270, 'क': 271, 'ख': 272, 'ः': 273, 'ḥ': 274, '緣': 275, '起': 276, 'ñ': 277, 'ब': 278, 'ॊ': 279, 'ि': 280, '\\n': 281, '安': 282, '樂': 283, '淨': 284, '土': 285, 'ṭ': 286, 'य': 287, 'ा': 288, 'न': 289, '禅': 290, '臨': 291, '済': 292, '宗': 293, '曹': 294, '洞': 295, '公': 296, 'ṛ': 297, 'ḍ': 298, '\\u200d': 299, '\\u200c': 300, '*': 301, 'ŵ': 302, '‘': 303, 'ễ': 304, 'ấ': 305, 'ũ': 306, 'Đ': 307, 'ế': 308, 'ă': 309, 'ả': 310, 'ø': 311, '\\u3000': 312, 'Φ': 313, '总': 314, '理': 315, 'ŏ': 316, 'ĭ': 317, 'π': 318, 'ο': 319, 'λ': 320, 'ύ': 321, 'ú': 322, 'ý': 323, 'τ': 324, 'ε': 325, 'χ': 326, 'ν': 327, 'ι': 328, 'κ': 329, 'ό': 330, 'ς': 331, 'İ': 332, '•': 333, '`': 334, 'Å': 335, 'Α': 336, 'ώ': 337, 'α': 338, 'Τ': 339, 'γ': 340, 'ά': 341, 'Ε': 342, 'δ': 343, 'υ': 344, 'Ι': 345, 'ρ': 346, 'μ': 347, 'Ν': 348, '業': 349, '学': 350, 'Š': 351, 'ë': 352, 'σ': 353, 'β': 354, 'ί': 355, 'ω': 356, 'ć': 357, 'ź': 358, 'â': 359, 'د': 360, 'ر': 361, 'ب': 362, 'ا': 363, '<': 364, '§': 365, '⁄': 366, 'ἀ': 367, 'έ': 368, 'Ç': 369, 'ˌ': 370, 'ɡ': 371, 'ɐ': 372, '̯': 373, 'ʏ': 374, '̩': 375, '…': 376, 'θ': 377, 'ἵ': 378, 'ὀ': 379, 'ξ': 380, '·': 381, 'Χ': 382, 'מ': 383, 'ָ': 384, 'ש': 385, 'ִ': 386, 'ׁ': 387, 'י': 388, 'ח': 389, 'ַ': 390, 'נ': 391, 'ו': 392, 'ּ': 393, 'צ': 394, 'ְ': 395, 'ר': 396, 'ה': 397, 'ד': 398, 'ם': 399, 'ن': 400, 'ص': 401, 'ي': 402, 'ى': 403, 'م': 404, 'س': 405, 'ح': 406, 'Ṣ': 407, 'ل': 408, 'ف': 409, 'ج': 410, 'ّ': 411, 'ة': 412, 'ی': 413, 'ت': 414, 'ई': 415, 'स': 416, 'ع': 417, 'ئ': 418, '\\u200e': 419, '基': 420, '督': 421, '徒': 422, 'ơ': 423, 'đ': 424, 'ố': 425, 'ồ': 426, '吉': 427, '利': 428, '丹': 429, '切': 430, 'キ': 431, 'シ': 432, 'タ': 433, '教': 434, 'ク': 435, 'チ': 436, 'ャ': 437, '기': 438, '독': 439, '교': 440, '도': 441, '그': 442, '리': 443, '스': 444, 'В': 445, 'е': 446, 'л': 447, 'и': 448, 'к': 449, 'о': 450, 'н': 451, 'я': 452, 'ж': 453, 'с': 454, 'т': 455, 'в': 456, 'Р': 457, 'у': 458, 'х': 459, 'р': 460, 'а': 461, 'ь': 462, 'п': 463, 'з': 464, 'й': 465, 'С': 466, 'Ф': 467, 'д': 468, 'ц': 469, 'ч': 470, 'б': 471, 'œ': 472, 'î': 473, 'К': 474, 'м': 475, 'ʰ': 476, '⟨': 477, '◌': 478, '⟩': 479, 'ʻ': 480, '˭': 481, 'ɕ': 482, 'ʂ': 483, 'ʱ': 484, '͡': 485, 'ɢ': 486, 'ᶢ': 487, 'ʘ': 488, 'ǀ': 489, 'ǁ': 490, 'ǃ': 491, 'ǂ': 492, '陽': 493, 'ψ': 494, 'ʷ': 495, 'ð': 496, 'ʔ': 497, 'ɦ': 498, '̤': 499, '♠': 500, 'ὑ': 501, 'ή': 502, '=': 503, '北': 504, '斗': 505, '卫': 506, '星': 507, '导': 508, '航': 509, '系': 510, '统': 511, '衛': 512, '導': 513, '統': 514, 'ě': 515, 'ǒ': 516, 'ǎ': 517, '试': 518, '验': 519, '試': 520, '驗': 521, 'ق': 522, 'و': 523, 'ק': 524, 'ὠ': 525, 'ē': 526, 'Π': 527, 'η': 528, 'ɫ': 529, '∅': 530, 'ˑ': 531, 'õ': 532, 'ž': 533, 'Ä': 534, 'ɤ': 535, 'ъ': 536, '̞': 537, 'ы': 538, 'Х': 539, '×': 540, '_': 541, '′': 542, '南': 543, '京': 544, '江': 545, '寧': 546, '宁': 547, '金': 548, '陵': 549, '冶': 550, '城': 551, '越': 552, '邑': 553, '秣': 554, '康': 555, '昇': 556, '州': 557, '渤': 558, '泥': 559, '天': 560, '下': 561, '淮': 562, '浙': 563, '東': 564, '长': 565, '域': 566, '三': 567, '火': 568, '炉': 569, '鄴': 570, 'א': 571, 'פ': 572, 'ך': 573, 'ז': 574, 'ל': 575, 'ע': 576, 'ט': 577, 'ן': 578, 'ײ': 579, 'ֿ': 580, 'ţ': 581, '̥': 582, '″': 583, '\\ufeff': 584, '´': 585, 'ѣ': 586, 'Σ': 587, 'ῖ': 588, 'ḱ': 589, 'ῆ': 590, '^': 591, 'č': 592, 'ę': 593, 'Б': 594, 'А': 595, 'М': 596, 'Ј': 597, 'Т': 598, 'ș': 599, 'Ž': 600, '̧': 601, 'ļ': 602, 'Á': 603, '̄': 604, 'ė': 605, 'ʒ': 606, 'ᵻ': 607, '→': 608, '‑': 609, '折': 610, '氵': 611, '人': 612, '歌': 613, '女': 614, 'ǚ': 615, '魏': 616, '蜀': 617, '惰': 618, '劇': 619, '上': 620, '有': 621, '堂': 622, '，': 623, '苏': 624, '杭': 625, 'ф': 626, 'Î': 627, 'φ': 628, 'ḗ': 629, 'ı': 630, '₹': 631, '\\u202f': 632, 'Г': 633, 'Δ': 634, 'Є': 635, 'З': 636, 'И': 637, 'إ': 638, 'ʿ': 639, 'û': 640, '♯': 641, 'ת': 642, 'ס': 643, 'ň': 644, 'þ': 645, '₂': 646, 'ą': 647, '₥': 648, 'ታ': 649, 'ላ': 650, 'ሪ': 651, 'ß': 652, 'Æ': 653, 'ƿ': 654, '⁊': 655, 'ċ': 656, 'ġ': 657, 'µ': 658, '\\u2009': 659, '{': 660, '}': 661, '|': 662, 'å': 663, 'ζ': 664, 'ş': 665, 'Μ': 666, 'Θ': 667, 'ῦ': 668, 'Ρ': 669, 'Υ': 670, '@': 671, 'П': 672, '́': 673, 'ш': 674, '̪': 675, 'आ': 676, 'ष': 677, '剎': 678, '那': 679, 'ण': 680, 'ቡ': 681, 'ን': 682, 'ሻ': 683, 'ሂ': 684, 'Ἐ': 685, 'ὰ': 686, 'ἐ': 687, '‚': 688, 'Ô': 689, '›': 690, 'フ': 691, 'ァ': 692, 'ミ': 693, 'ー': 694, 'コ': 695, 'ピ': 696, 'ュ': 697, '현': 698, '대': 699, '컴': 700, '보': 701, '이': 702, '仕': 703, '様': 704, 'Д': 705, '小': 706, '才': 707, 'Ἀ': 708, 'Β': 709, 'ὶ': 710, 'ὸ': 711, 'ÿ': 712, 'ő': 713, 'Ó': 714, 'خ': 715, 'أ': 716, 'ʾ': 717, 'ἄ': 718, 'ׂ': 719, 'ֵ': 720, 'ِ': 721, 'ْ': 722, 'َ': 723, 'ʼ': 724, 'Ἰ': 725, 'ỉ': 726, 'ꜣ': 727, 'Ἑ': 728, 'ﬁ': 729, 'ﬂ': 730, '手': 731, '羽': 732, '先': 733, 'ɣ': 734, '한': 735, '국': 736, '전': 737, '쟁': 738, '韓': 739, '戰': 740, '爭': 741, '해': 742, '방': 743, '抗': 744, '美': 745, '战': 746, '争': 747, '鮮': 748, '韩': 749, 'Γ': 750, 'Λ': 751, 'Κ': 752, 'ù': 753, 'Η': 754, 'Ψ': 755, 'Í': 756, '福': 757, '話': 758, '话': 759, '̍': 760, '˥': 761, '˨': 762, '˩': 763, '泉': 764, '漳': 765, '片': 766, '閩': 767, '語': 768, '闽': 769, '语': 770, 'ǐ': 771, 'ǔ': 772, '佬': 773, '陳': 774, '政': 775, '光': 776, '潮': 777, '審': 778, '知': 779, '竹': 780, '文': 781, '白': 782, '異': 783, '讀': 784, '音': 785, '漢': 786, '字': 787, '替': 788, '媠': 789, '婎': 790, '高': 791, '懸': 792, '毋': 793, '呣': 794, '唔': 795, '𪜶': 796, '肉': 797, '呷': 798, '食': 799, '言': 800, '普': 801, '通': 802, '詞': 803, '典': 804, 'Ú': 805, 'ロ': 806, 'レ': 807, '¡': 808, '√': 809, 'Ć': 810, 'г': 811, '예': 812, '수': 813, '장': 814, '로': 815, '회': 816, '고': 817, '려': 818, '파': 819, '신': 820, '통': 821, '합': 822, '동': 823, 'Ḥ': 824, '¿': 825, 'န': 826, 'ာ': 827, 'း': 828, 'သ': 829, 'ه': 830, 'ُ': 831, 'Е': 832, 'Ď': 833, 'ɥ': 834, 'ț': 835, 'Ẓ': 836, 'ء': 837, 'ك': 838, 'ش': 839, '汉': 840, 'ữ': 841, '河': 842, '湖': 843, '沖': 844, '滑': 845, 'ŋ': 846, '中': 847, '蝴': 848, '蝶': 849, '虫': 850, '批': 851, '把': 852, '枇': 853, '杷': 854, '琵': 855, '琶': 856, '八': 857, '分': 858, '章': 859, '草': 860, '隶': 861, '隸': 862, '今': 863, '頓': 864, '顿': 865, '松': 866, '木': 867, '粁': 868, '米': 869, '千': 870, '㓾': 871, '常': 872, '用': 873, '標': 874, '準': 875, '體': 876, '表': 877, '次': 878, '现': 879, '代': 880, '平': 881, '考': 882, '他': 883, '她': 884, '牠': 885, '它': 886, '祂': 887, '和': 888, '咊': 889, '龢': 890, '齉': 891, '龘': 892, '籲': 893, '鬱': 894, '憂': 895, '豔': 896, '釁': 897, '挑': 898, '鱻': 899, '𪚥': 900, '龍': 901, '𠔻': 902, '興': 903, '受': 904, '又': 905, '祐': 906, '菩': 907, '薩': 908, '萨': 909, '十': 910, '厘': 911, '瓦': 912, '瓩': 913, '糎': 914, '圕': 915, '圖': 916, '書': 917, '館': 918, '社': 919, '会': 920, '主': 921, '义': 922, '礻': 923, '囍': 924, '喜': 925, '双': 926, '雙': 927, '廿': 928, '二': 929, '卅': 930, '卌': 931, '四': 932, '七': 933, '门': 934, '問': 935, '題': 936, '问': 937, '题': 938, '合': 939, '体': 940, '节': 941, '節': 942, '珊': 943, '瑚': 944, '胡': 945, '块': 946, '塊': 947, '篆': 948, '新': 949, '旧': 950, '当': 951, 'た': 952, 'つ': 953, '竜': 954, '來': 955, '来': 956, '雲': 957, '云': 958, '雨': 959, '叠': 960, '覆': 961, '像': 962, '물': 963, '사': 964, '람': 965, '인': 966, '큰': 967, '작': 968, '을': 969, '소': 970, '아': 971, '래': 972, '하': 973, '비': 974, '부': 975, '父': 976, '나': 977, '라': 978, '름': 979, '风': 980, '动': 981, '仪': 982, 'デ': 983, 'ジ': 984, 'モ': 985, 'マ': 986, 'メ': 987, '˚': 988, '妹': 989, '仔': 990, '時': 991, '都': 992, '院': 993, '庁': 994, 'い': 995, 'ろ': 996, 'は': 997, '良': 998, 'Н': 999, 'ņ': 1000, 'Ş': 1001, 'ב': 1002, 'ג': 1003, 'ץ': 1004, 'ħ': 1005, '巴': 1006, '杜': 1007, '宇': 1008, '西': 1009, '成': 1010, '经': 1011, '济': 1012, '技': 1013, '术': 1014, '开': 1015, '发': 1016, '区': 1017, '产': 1018, '业': 1019, 'ꀕ': 1020, '︘': 1021, '�': 1022, '～': 1023, '〜': 1024, 'เ': 1025, 'แ': 1026, 'โ': 1027, 'ใ': 1028, 'ไ': 1029, 'ส': 1030, 'ด': 1031, 'ง': 1032, 'ˤ': 1033, '̀': 1034, '藏': 1035, 'བ': 1036, 'ོ': 1037, 'ད': 1038, '་': 1039, '˧': 1040, '吐': 1041, '蕃': 1042, '番': 1043, '烏': 1044, '斯': 1045, '伯': 1046, '特': 1047, '唐': 1048, '古': 1049, '忒': 1050, '图': 1051, 'Œ': 1052, 'Ꭰ': 1053, 'Ꮝ': 1054, 'Ꭶ': 1055, 'Ꮿ': 1056, 'Ꭹ': 1057, 'Ꭸ': 1058, 'Ᏹ': 1059, 'Ꭳ': 1060, 'Ꮃ': 1061, 'Ꮀ': 1062, 'Ꮉ': 1063, 'ˀ': 1064, 'ग': 1065, 'ḷ': 1066, 'ಚ': 1067, 'ಾ': 1068, 'ಲ': 1069, 'ು': 1070, 'ಕ': 1071, '್': 1072, 'ಯ': 1073, 'ರ': 1074, 'ɭ': 1075, 'প': 1076, 'া': 1077, 'ল': 1078, 'স': 1079, 'ম': 1080, '্': 1081, 'র': 1082, 'জ': 1083, 'য': 1084, 'ಪ': 1085, 'ಶ': 1086, 'ಿ': 1087, 'ಮ': 1088, 'ಸ': 1089, 'ಜ': 1090, 'ʕ': 1091, '≥': 1092, 'ℛ': 1093, 'ɸ': 1094, '∗': 1095, '∈': 1096, '≡': 1097, '∖': 1098, 'ʀ': 1099, 'ǥ': 1100, 'ǵ': 1101, 'Ø': 1102, 'Õ': 1103, 'ἔ': 1104, 'Â': 1105, 'Ἄ': 1106, 'ἁ': 1107, 'ἰ': 1108, 'ἴ': 1109, 'Ἥ': 1110, 'ἡ': 1111, '𐀞': 1112, '𐀊': 1113, '𐀍': 1114, '𐀚': 1115, 'ὐ': 1116, 'ἑ': 1117, 'ἶ': 1118, 'ῶ': 1119, 'ὲ': 1120, '武': 1121, '士': 1122, '家': 1123, '侍': 1124, '団': 1125, '制': 1126, '征': 1127, '夷': 1128, '将': 1129, '入': 1130, 'り': 1131, '伐': 1132, '斬': 1133, '捨': 1134, 'て': 1135, '御': 1136, '免': 1137, '織': 1138, '田': 1139, '総': 1140, '介': 1141, '郎': 1142, '信': 1143, '浦': 1144, '按': 1145, '針': 1146, '逸': 1147, '見': 1148, '洲': 1149, '耶': 1150, '楊': 1151, '印': 1152, '船': 1153, '兵': 1154, '守': 1155, 'ら': 1156, 'ふ': 1157, 'る': 1158, 'さ': 1159, 'ぶ': 1160, 'ひ': 1161, 'む': 1162, '集': 1163, '腹': 1164, '道': 1165, '豊': 1166, '臣': 1167, '秀': 1168, '≈': 1169, '⋅': 1170, 'ɨ': 1171, '青': 1172, '藍': 1173, '綠': 1174, 'ǜ': 1175, '緑': 1176, 'グ': 1177, 'ờ': 1178, 'ụ': 1179, 'ข': 1180, 'ี': 1181, 'ย': 1182, 'ว': 1183, 'گ': 1184, 'Č': 1185, '№': 1186, '÷': 1187, 'ٔ': 1188, 'ث': 1189, 'ğ': 1190, 'ظ': 1191, 'ض': 1192, '¶': 1193, 'Ġ': 1194, 'Ħ': 1195, 'ẓ': 1196, 'Ḍ': 1197, 'ร': 1198, 'น': 1199, 'ศ': 1200, 'า': 1201, 'อ': 1202, 'ิ': 1203, 'ล': 1204, 'ม': 1205, 'ề': 1206, 'ử': 1207, 'ớ': 1208, 'ư': 1209, 'ʎ': 1210, 'ˇ': 1211, 'ť': 1212, 'ď': 1213, 'Ꮤ': 1214, 'Ꮎ': 1215, 'Ꮟ': 1216, '₤': 1217, 'ذ': 1218, 'ɯ': 1219, '♆': 1220, '海': 1221, '⅓': 1222, '前': 1223, '东': 1224, '後': 1225, '比': 1226, '蒲': 1227, '奴': 1228, '檀': 1229, '石': 1230, '槐': 1231, '李': 1232, '閏': 1233, '膺': 1234, '甫': 1235, '張': 1236, '奐': 1237, '何': 1238, '苗': 1239, '博': 1240, '魂': 1241, '魄': 1242, '神': 1243, '廷': 1244, '議': 1245, '尉': 1246, '部': 1247, '曲': 1248, '五': 1249, '銖': 1250, '趙': 1251, '過': 1252, '凹': 1253, '匠': 1254, '丁': 1255, '緩': 1256, 'ܩ': 1257, 'ܪ': 1258, 'ܝ': 1259, 'ܢ': 1260, 'ܐ': 1261, '∝': 1262, '¼': 1263, '亜': 1264, '戦': 1265, '日': 1266, '事': 1267, '変': 1268, 'ٍ': 1269, 'Ē': 1270, 'ἱ': 1271, 'і': 1272, 'ў': 1273, 'ŭ': 1274, 'ї': 1275, 'パ': 1276, '슈': 1277, '퍼': 1278, 'カ': 1279, 'ッ': 1280, 'ĝ': 1281, '𒌦': 1282, '𒊕': 1283, '𒈪': 1284, '𒂵': 1285, 'ʲ': 1286, 'ѵ': 1287, 'ѳ': 1288, 'ѫ': 1289, 'ѭ': 1290, 'ю': 1291, 'ѧ': 1292, 'ѩ': 1293, 'щ': 1294, 'О': 1295, 'э': 1296, 'ё': 1297, 'ɵ': 1298, 'Л': 1299, 'ֹ': 1300, '清': 1301, '̌': 1302, '明': 1303, '月': 1304, '之': 1305, '劉': 1306, '佐': 1307, '義': 1308, '皇': 1309, '帝': 1310, '親': 1311, '藩': 1312, '澄': 1313, '弘': 1314, '桓': 1315, '雍': 1316, '巡': 1317, '撫': 1318, '提': 1319, '總': 1320, '從': 1321, '關': 1322, '皆': 1323, '惡': 1324, '廚': 1325, '陋': 1326, '習': 1327, '。': 1328, '只': 1329, '可': 1330, '於': 1331, '門': 1332, '司': 1333, '境': 1334, '红': 1335, '紅': 1336, '包': 1337, 'Ⲭ': 1338, 'ⲏ': 1339, 'ⲙ': 1340, 'ⲓ': 1341, '̠': 1342, '𒆳': 1343, '𒄑': 1344, '𒊒': 1345, '₯': 1346, 'Հ': 1347, 'ա': 1348, 'յ': 1349, 'ս': 1350, 'տ': 1351, 'ն': 1352, 'ց': 1353, 'ի': 1354, 'Պ': 1355, 'ր': 1356, 'կ': 1357, 'հ': 1358, 'Լ': 1359, 'բ': 1360, 'ҷ': 1361, 'Ҷ': 1362, 'ҳ': 1363, 'ـ': 1364, 'ӣ': 1365, 'پ': 1366, 'ʈ': 1367, 'ɳ': 1368, 'ɖ': 1369, 'ठ': 1370, 'ड': 1371, 'प': 1372, 'त': 1373, '≠': 1374, '⊆': 1375, '℞': 1376, '號': 1377, '詔': 1378, '哉': 1379, '乾': 1380, '黑': 1381, '馬': 1382, '蕭': 1383, '札': 1384, '剌': 1385, '抹': 1386, '孛': 1387, '迭': 1388, '兒': 1389, '塔': 1390, '已': 1391, '刺': 1392, '史': 1393, '秉': 1394, '直': 1395, '柔': 1396, '嚴': 1397, '實': 1398, '鈔': 1399, 'ạ': 1400, 'ằ': 1401, 'ầ': 1402, 'ệ': 1403, '尚': 1404, '省': 1405, '奎': 1406, '閣': 1407, '學': 1408, '經': 1409, '世': 1410, '太': 1411, '祖': 1412, '樞': 1413, '密': 1414, '授': 1415, '暦': 1416, 'Ῥ': 1417, '--NULL--': 0, '--OOV--': 1418}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must use keyword argument for key function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0ce17f82d77d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar2idx_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must use keyword argument for key function"
     ]
    }
   ],
   "source": [
    "print(char2idx_dict)\n",
    "sorted(char_counter.items(), lambda x, y: cmp(x[1], y[1]), reverse=True)\n",
    "print(char_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_indexs(exa, word2idx_dict, char2idx_dict, cont_limit=400, ques_limit=50, ans_limit=30, char_limit=16):\n",
    "    n = len(exa) #total number of questions, >130k if all\n",
    "    miss_word = 0\n",
    "    miss_char = 0\n",
    "    overlimit = 0\n",
    "    #outputs are:\n",
    "    cont_index = np.zeros((n, cont_limit)) \n",
    "    ques_index = np.zeros((n, ques_limit))\n",
    "    cont_char_index = np.zeros((n, cont_limit, char_limit))\n",
    "    ques_char_index = np.zeros((n, ques_limit, char_limit))\n",
    "    cont_len = np.zeros((n, 1))\n",
    "    ques_len = np.zeros((n, 1))\n",
    "    y_start = np.zeros((n, cont_limit))\n",
    "    y_end = np.zeros((n, cont_limit))\n",
    "    qid = np.zeros((n))\n",
    "    \n",
    "    \n",
    "    #contexte\n",
    "    for i in tqdm(range(n-1)):\n",
    "        qid[i] = int(exa[i]['id'])\n",
    "        \n",
    "        contexts = exa[i]['context_tokens']\n",
    "        cont_len[i,0] = min(cont_limit, len(contexts))\n",
    "        for j,c in enumerate(contexts):\n",
    "            if j >= cont_limit:\n",
    "                break\n",
    "            if c in word2idx_dict:\n",
    "                cont_index[i,j] = word2idx_dict[c]\n",
    "            else:\n",
    "                miss_word += 1\n",
    "                cont_index[i,j] = word2idx_dict['--OOV--']\n",
    "        contexts_char = exa[i]['context_chars']\n",
    "        for j,c in enumerate(contexts_char):\n",
    "            if j >= cont_limit:\n",
    "                break\n",
    "            for j2,c2 in enumerate(c):\n",
    "                if j2 >= char_limit:\n",
    "                    break\n",
    "                if c2 in char2idx_dict:\n",
    "                    cont_char_index[i,j,j2] = char2idx_dict[c2]\n",
    "                else:\n",
    "                    miss_char += 1\n",
    "                    cont_char_index[i,j,j2] = char2idx_dict['--OOV--']\n",
    "        #answer\n",
    "        try:\n",
    "            st = exa[i]['y1s'][0]\n",
    "            ed = exa[i]['y2s'][0]\n",
    "            if st < cont_limit:\n",
    "                y_start[i, st] = 1\n",
    "            if ed < cont_limit:\n",
    "                if ed-st > ans_limit:\n",
    "                    y_end[i, st + ans_limit] = 1\n",
    "                    overlimit += 1\n",
    "                else:\n",
    "                    y_end[i, ed] = 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #question\n",
    "        contexts = exa[i]['ques_tokens']\n",
    "        ques_len[i, 0] = min(ques_limit, len(contexts))\n",
    "        for j,c in enumerate(contexts):\n",
    "            if j >= ques_limit:\n",
    "                break\n",
    "            if c in word2idx_dict:\n",
    "                ques_index[i, j] = word2idx_dict[c]\n",
    "            else:\n",
    "                miss_word += 1\n",
    "                ques_index[i, j] = word2idx_dict['--OOV--']\n",
    "        contexts_char = exa[i]['ques_chars']\n",
    "        for j,c in enumerate(contexts_char):\n",
    "            if j >= ques_limit:\n",
    "                break\n",
    "            for j2,c2 in enumerate(c):\n",
    "                if j2 >= char_limit:\n",
    "                    break\n",
    "                if c2 in char2idx_dict:\n",
    "                    ques_char_index[i, j, j2] = char2idx_dict[c2]\n",
    "                else:\n",
    "                    miss_char += 1\n",
    "                    ques_char_index[i,j,j2] = char2idx_dict['--OOV--']\n",
    "    print('miss word:', miss_word)\n",
    "    print('miss char:', miss_char)\n",
    "    print('over limit:', overlimit)\n",
    "        \n",
    "    return cont_index, ques_index, cont_char_index, ques_char_index, cont_len, ques_len, y_start, y_end, qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1664.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss word: 313\n",
      "miss char: 0\n",
      "over limit: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#same reapeated 3 times for train dev and test\n",
    "#1st get indices\n",
    "#\n",
    "contw_input, quesw_input, contc_input,\\\n",
    "quesc_input, cont_len, ques_len, y_start,\\\n",
    "y_end, qid = get_indexs(train_examples, word2idx_dict, char2idx_dict)\n",
    "\n",
    "np.save('dataset/train_contw_input.npy',contw_input)\n",
    "np.save('dataset/train_quesw_input.npy',quesw_input)\n",
    "np.save('dataset/train_contc_input.npy',contc_input)\n",
    "np.save('dataset/train_quesc_input.npy',quesc_input)\n",
    "np.save('dataset/train_cont_len.npy',cont_len)\n",
    "np.save('dataset/train_ques_len.npy',ques_len)\n",
    "np.save('dataset/train_y_start.npy',y_start)\n",
    "np.save('dataset/train_y_end.npy',y_end)\n",
    "np.save('dataset/train_qid.npy',qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1390.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss word: 399\n",
      "miss char: 0\n",
      "over limit: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "contw_input, quesw_input, contc_input, quesc_input, cont_len, ques_len, y_start, y_end, qid\\\n",
    "=get_indexs(dev_examples, word2idx_dict, char2idx_dict)\n",
    "\n",
    "np.save('dataset/dev_contw_input.npy',contw_input)\n",
    "np.save('dataset/dev_quesw_input.npy',quesw_input)\n",
    "np.save('dataset/dev_contc_input.npy',contc_input)\n",
    "np.save('dataset/dev_quesc_input.npy',quesc_input)\n",
    "np.save('dataset/dev_cont_len.npy',cont_len)\n",
    "np.save('dataset/dev_ques_len.npy',ques_len)\n",
    "np.save('dataset/dev_y_start.npy',y_start)\n",
    "np.save('dataset/dev_y_end.npy',y_end)\n",
    "np.save('dataset/dev_qid.npy',qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1414.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss word: 398\n",
      "miss char: 0\n",
      "over limit: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "contw_input, quesw_input, contc_input, quesc_input, cont_len, ques_len, y_start, y_end, qid\\\n",
    "=get_indexs(test_examples, word2idx_dict, char2idx_dict)\n",
    "\n",
    "np.save('dataset/test_contw_input.npy',contw_input)\n",
    "np.save('dataset/test_quesw_input.npy',quesw_input)\n",
    "np.save('dataset/test_contc_input.npy',contc_input)\n",
    "np.save('dataset/test_quesc_input.npy',quesc_input)\n",
    "np.save('dataset/test_cont_len.npy',cont_len)\n",
    "np.save('dataset/test_ques_len.npy',ques_len)\n",
    "np.save('dataset/test_y_start.npy',y_start)\n",
    "np.save('dataset/test_y_end.npy',y_end)\n",
    "np.save('dataset/test_qid.npy',qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
